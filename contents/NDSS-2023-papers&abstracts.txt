{"a robust counting sketch for data plane intrusion detection": "demands are increasing to measure per-flow statistics in the data plane of high-speed switches. however, the resource constraint of the data plane is the biggest challenge. although existing in-data plane solutions improve memory efficiency by accommodating zipfian distribution of network traffic, they cannot adapt to various flow size distributions due to their static data structure. in other words, they cannot provide robust flow measurement under complex traffic patterns (e.g. under attacks).  recent works suggest dynamic data structure management schemes, but the high complexity is the major obstruction for the data plane deployment. in this paper, we present count-less sketch that enables robust and accurate network measurement under a wide variety of traffic distributions without dynamic data structure update. count-less applies a novel sketch update strategy, called {em minimum update}, which approximates the conservative update strategy of count-min for fitting into in-network switches. not only theoretical proof on count-less's estimation but also comprehensive experimental results are presented in terms of estimation accuracy and throughput of count-less, compared to count-min (baseline), elastic sketch, and fcm sketch. more specifically, experiment results on security applications including estimation errors under various skewness parameters are provided. count-less is much more accurate in all measurement tasks than count-min and outperforms fcm sketch and elastic sketch, state-of-the-art algorithms without the help of any special hardware like tcam. to prove its feasibility in the data plane of a high-speed switch, count-less prototype on an asic-based programmable switch (tofino) is implemented in p4 language and evaluated. in terms of data plane latency, count-less is 1.53x faster than fcm, while consuming 1.56x less resources such as hash bits, sram, and alu of a programmable switch.", "a systematic study of the consistency of two-factor authentication user journeys on top-ranked websites": "heuristics for user experience state that users will transfer their expectations from one product to another. a lack of consistency between products can increase users' cognitive friction, leading to frustration and rejection. this paper presents the first systematic study of the external, functional consistency of two-factor authentication user journeys on top-ranked websites. we find that these websites implement only a minimal number of design aspects consistently (e.g., naming and location of settings) but exhibit mixed design patterns for setup and usage of a second factor. moreover, we find that some of the more consistently realized aspects, such as descriptions of two-factor authentication, have been described in the literature as problematic and adverse to user experience. our results advocate for more general ux guidelines for 2fa implementers and raise new research questions about the 2fa user journeys.", "an os-agnostic approach to memory forensics": "the analysis of memory dumps presents unique challenges, as operating systems use a variety of (often undocumented) ways to represent data in memory. to solve this problem, forensics tools maintain collections of models that precisely describe the kernel data structures used by a handful of operating systems. however, these models cannot be generalized and developing new models may require a very long and tedious reverse engineering effort for closed source systems. in the last years, the tremendous increase in the number of iot devices, smart-home appliances and cloud-hosted vms resulted in a growing number of oss which are not supported by current forensics tools. the way we have been doing memory forensics until today, based on handwritten models and rules, cannot simply keep pace with this variety of systems.\nto overcome this problem, in this paper we introduce the new concept of emph{os-agnostic memory forensics}, which is based on techniques that can recover certain forensics information without emph{any} knowledge of the internals of the underlying os. our approach allows to automatically identify different types of data structures by using only their topological constraints and then supports two modes of investigation. in the first, it allows to traverse the recovered structures by starting from predetermined textit{seeds}, i.e., pieces of forensics-relevant information (such as a process name or an ip address) that an analyst knows emph{a priori} or that can be easily identified in the dump. our experiments show that even a single seed can be sufficient to recover the entire list of processes and other important forensics data structures in dumps obtained from 14 different oss, without any knowledge of the underlying kernels. in the second mode of operation, our system requires no seed but instead uses a set of heuristics to rank all memory data structures and present to the analysts only the most `promising' ones. even in this case, our experiments show that an analyst can use our approach to easily identify forensics-relevant structured information in a truly os-agnostic scenario.", "authentisense: a scalable behavioral biometrics authentication scheme using few-shot learning for mobile platforms": "mobile applications are widely used for online services sharing a large amount of personal data online. one-time authentication techniques such as passwords and physiological biometrics (e.g., fingerprint, face, and iris) have their own advantages but also disadvantages since they can be stolen or emulated, and do not prevent access to the underlying device, once it is unlocked. to address these challenges, complementary authentication systems based on behavioural biometrics have emerged. the goal is to continuously profile users based on their interaction with the mobile device. however, existing behavioural authentication schemes are not (i) user-agnostic meaning that they cannot dynamically handle changes in the user-base without model re-training, or (ii) do not scale well to authenticate millions of users.\nin this paper, we present authentisense, a user-agnostic, scalable, and efficient behavioural biometrics authentication system that enables continuous authentication and utilizes only motion patterns (i.e., accelerometer, gyroscope, and magnetometer data) while users interact with mobile apps. our approach requires neither manually engineered features nor a significant amount of data for model training. we leverage a few-shot learning technique, called siamese network, to authenticate users at a large scale. we perform a systematic measurement study and report the impact of the parameters such as interaction time needed for authentication and n-shot verification (comparison with enrollment samples) at the recognition stage. remarkably, authentisense achieves high accuracy of up to 97% in terms of f1-score even when evaluated in a few-shot fashion that requires only a few behaviour samples per user (3 shots). our approach accurately authenticates users only after 1 second of user interaction. for authentisense, we report a far and frr of 0.023 and 0.057, respectively.", "automata-based automated detection of state machine bugs in protocol implementations": "implementations of stateful security protocols\nmust carefully manage the type and order of exchanged messages and cryptographic material,\nby maintaining a state machine which keeps track of protocol progress.\ncorresponding implementation flaws, called\nemph{state machine bugs}, can constitute serious security vulnerabilities.\nwe present an automated black-box technique for detecting state machine bugs in implementations of stateful network protocols.\nit takes as input a catalogue of state machine bugs for the protocol, each specified as a finite automaton which accepts\nsequences of messages that exhibit the bug, and a (possibly inaccurate) model of the implementation under test,\ntypically obtained by model learning.\nour technique constructs the set of sequences that (according to the model) can be performed by the implementation and\nthat (according to the automaton) expose the bug.\nthese sequences are then transformed to test cases on the actual implementation to find a witness for the bug or filter out false alarms.\nwe have applied our technique on three widely-used implementations of ssh servers and nine different dtls server and client implementations, including their most recent versions.\nour technique easily reproduced all bugs identified by security researchers before,\nand produced witnesses for them.\nmore importantly, it revealed several previously unknown bugs in the same implementations,\ntwo new vulnerabilities, and a variety of new bugs and non-conformance issues\nin newer versions of the same ssh and dtls implementations.", "binaryinferno: a semantic-driven approach to field inference for binary message formats": "we present binaryinferno, a fully automatic tool for reverse engineering binary message formats. given a set of messages with the same format, the tool uses an ensemble of detectors to infer a collection of partial descriptions and then automatically integrates the partial descriptions into a semantically-meaningful description that can be used to parse future packets with the same format.  as its ensemble, binaryinferno uses a modular and extensible set of targeted detectors, including detectors for identifying atomic data types such as ieee floats, timestamps, and integer length fields; for finding boundaries between adjacent fields using shannon entropy; and for discovering variable-length sequences by searching for common serialization idioms. we evaluate binaryinferno's performance on sets of packets drawn from 10 binary protocols. our semantic-driven approach significantly decreases false positive rates and increases precision when compared to the previous state of the art. for top-level protocols we identify field boundaries with an average precision of 0.69, an average recall of 0.73, and an average false positive rate of 0.04, significantly outperforming five other state-of-the-art protocol reverse engineering tools on the same data sets: awre (0.18, 0.03, 0.04), fieldhunter (0.68, 0.37, 0.01), nemesys (0.31, 0.44, 0.11), netplier (0.29, 0.75, 0.22), and netzob (0.57, 0.42, 0.03). we believe our improvements in precision and false positive rates represent what our target user most wants: semantically meaningful descriptions with fewer false positives.", "brokenwire : wireless disruption of ccs electric vehicle charging": "we present a novel attack against the combined charging system, one of the most widely used dc rapid charging technologies for electric vehicles (evs). our attack, brokenwire, interrupts necessary control communication between the vehicle and charger, causing charging sessions to abort. the attack requires only temporary physical proximity and can be conducted wirelessly from a distance, allowing individual vehicles or entire fleets to be disrupted stealthily and simultaneously. in addition, it can be mounted with off-the-shelf radio hardware and minimal technical knowledge. by exploiting csma/ca behavior, only a very weak signal needs to be induced into the victim to disrupt communication \u2014 exceeding the effectiveness of broadband noise jamming by three orders of magnitude. the exploited behavior is a required part of the homeplug green phy, din 70121 & iso 15118 standards and all known implementations exhibit it.\nwe first study the attack in a controlled testbed and then demonstrate it against eight vehicles and 20 chargers in real deployments. we find the attack to be successful in the real world, at ranges up to 47 m, for a power budget of less than 1 w. we further show that the attack can work between the floors of a building (e.g., multi-story parking), through perimeter fences, and from 'drive-by' attacks. we present a heuristic model to estimate the number of vehicles that can be attacked simultaneously for a given output power.\nbrokenwire has immediate implications for a substantial proportion of the around 12 million battery evs on the roads worldwide \u2014 and profound effects on the new wave of electrification for vehicle fleets, both for private enterprise and crucial public services, as well as electric buses, trucks, and small ships. as such, we conducted a disclosure to the industry and discussed a range of mitigation techniques that could be deployed to limit the impact.", "browser permission mechanisms demystified": "modern web services provide rich content by accessing resources on user devices, including hardware devices such as cameras, microphones, and gpss.\nweb browser vendors have adopted permission mechanisms that achieve appropriate control over access to such resources to protect user privacy.\nthe permission mechanism gives users the ability to grant or deny their browser access to resources for each website.\ndespite the importance of permission mechanisms in protecting user privacy, previous studies have not been conducted to systematically understand their behavior and implementation.\nin this study, we developed permium, a web browser analysis framework that automatically analyzes the behavior of permission mechanisms implemented by various browsers.\nusing the permium framework, we systematically studied the behavior of permission mechanisms for 22 major browser implementations running on five different operating systems, including mobile and desktop.\nwe determined that the implementation and behavior of permission mechanisms are fragmented and inconsistent between operating systems, even for the same browser (i.e., windows chrome vs. ios chrome) and that the implementation inconsistencies can lead to privacy risks.\nbased on the behavior and implementation inconsistencies of the permission mechanism revealed by our measurement study, we developed two proof-of-concept attacks and evaluated their feasibility.\nthe first attack uses the permission information collected by exploiting the inconsistencies to secretly track the user.\nthe second attack aims to create a situation in which the user cannot correctly determine the origin of the permission request, and the user incorrectly grants permission to a malicious site.\nfinally, we clarify the technical issues that must be standardized in privacy mechanisms and provide recommendations to os/browser vendors to mitigate the threats identified in this study.", "chargeprint: a framework for internet-scale discovery and security analysis of ev charging management systems": "electric vehicle charging management systems (evcms) are a collection of specialized software that allow users to remotely operate electric vehicle charging stations (evcs). with the increasing number of deployed evcs to support the growing global ev fleet, the number of evcms are consequently growing, which introduces a new attack surface. in this paper, we propose a novel multi-stage framework, chargeprint, to discover internet-connected evcms and investigate their security posture. chargeprint leverages identifiers extracted from a small seed of evcms to extend the capabilities of device search engines through iterative fingerprinting and a combination of classification and clustering approaches. using initial seeds from 1,800 discovered hosts that deployed 9 distinct evcms, we identified 27,439 online evcs instrumented by 44 unique evcms. consequently, our in-depth security analysis highlights the insecurity of the deployed evcms by uncovering 120 0-day vulnerabilities, which shed light on the feasibility of cyber attacks against the evcs, its users, and the connected power grid. finally, while we recommend countermeasures to mitigate future threats, we contribute to the security of the evcs ecosystem by conducting a coordinated vulnerability disclosure (cvd) effort with system developers/vendors who acknowledged and assigned the discovered vulnerabilities more than 20 cve-ids.", "darwin: survival of the fittest fuzzing mutators": "fuzzing is an automated software testing technique broadly adopted by the industry. a popular variant is mutation-based fuzzing, which discovers a large number of bugs in practice. while the research community has studied mutation-based fuzzing for years now, the algorithms' interactions within the fuzzer are highly complex and can, together with the randomness in every instance of a fuzzer, lead to unpredictable effects. most efforts to improve this fragile interaction focused on optimizing seed scheduling. however, real-world results like google's fuzzbench highlight that these approaches do not consistently show improvements in practice. another approach to improve the fuzzing process algorithmically is optimizing mutation scheduling. unfortunately, existing mutation scheduling approaches also failed to convince because of missing real-world improvements or too many user-controlled parameters whose configuration requires expert knowledge about the target program. this leaves the challenging problem of cleverly processing test cases and achieving a measurable improvement unsolved. we present darwin, a novel mutation scheduler and the first to show fuzzing improvements in a realistic scenario without the need to introduce additional user-configurable parameters, opening this approach to the broad fuzzing community. darwin uses an evolution strategy to systematically optimize and adapt the probability distribution of the mutation operators during fuzzing.  we implemented a prototype based on the popular general-purpose fuzzer afl. darwin significantly outperforms the state-of-the-art mutation scheduler and the afl baseline in our own coverage experiment, in fuzzbench, and by finding 15 out of 21 bugs the fastest in the magma benchmark. finally, darwin found 20 unique bugs (including one novel bug), 66% more than afl, in widely-used real-world applications.", "detecting unknown encrypted malicious traffic in real time via flow interaction graph analysis": "nowadays traffic on the internet has been widely encrypted to protect its confidentiality and privacy. however, traffic encryption is always abused by attackers to conceal their malicious behaviors. since the encrypted malicious traffic has similar features to benign flows, it can easily evade traditional detection methods. particularly, the existing encrypted malicious traffic detection methods are supervised and they rely on the prior knowledge of known attacks (e.g., labeled datasets). detecting unknown encrypted malicious traffic in real time, which does not require prior domain knowledge, is still an open problem.\nin this paper, we propose hypervision, a realtime unsupervised machine learning (ml) based malicious traffic detection system. particularly, hypervision is able to detect unknown patterns of encrypted malicious traffic by utilizing a compact inmemory graph built upon the traffic patterns. the graph captures flow interaction patterns represented by the graph structural features, instead of the features of specific known attacks. we develop an unsupervised graph learning method to detect abnormal interaction patterns by analyzing the connectivity, sparsity, and statistical features of the graph, which allows hypervision to detect various encrypted attack traffic without requiring any labeled datasets of known attacks. moreover, we establish an information theory model to demonstrate that the information preserved by the graph approaches the ideal theoretical bound. we show the performance of hypervision by real-world experiments with 92 datasets including 48 attacks with encrypted malicious traffic. the experimental results illustrate that hypervision achieves at least 0.92 auc and 0.86 f1, which significantly outperform the state-of-the-art methods. in particular, more than 50% attacks in our experiments can evade all these methods. moreover, hypervision achieves at least 80.6 gb/s detection throughput with the average detection latency of 0.83s.", "efficient dynamic proof of retrievability for cold storage": "storage-as-a-service (staas) permits the client to outsource her data to the cloud thereby, reducing data management and maintenance costs. however, staas also brings significant data integrity and soundness concerns since the storage provider might not keep the client data intact and retrievable all the time (e.g., cost saving via deletions). proof of retrievability (por) can validate the integrity and retrievability of remote data effectively. this technique can be useful for regular audits to monitor data compromises, as well as to comply with standard data regulations. in particular, cold storage applications (e.g., ms azure, amazon glacier) require regular and frequent audits but with less frequent data modification. yet, despite their merits, existing por techniques generally focus on other metrics (e.g., low storage, fast update, metadata privacy) but not audit efficiency (e.g., low audit time, small proof size). hence, there is a need to develop new por techniques that achieve efficient data audit while preserving update and retrieval performance.\nin this paper, we propose porla, a new por framework that permits efficient data audit, update, and retrieval functionalities simultaneously. porla permits data audit in both private and public settings, each of which features asymptotically (and concretely) smaller audit-proof size and lower audit time than all the prior works while retaining the same asymptotic data update overhead. porla achieves all these properties by composing erasure codes with verifiable computation techniques which, to our knowledge, is a new approach to por design. we address several challenges that arise in such a composition by creating a new homomorphic authenticated commitment scheme, which can be of independent interest. we fully implemented porla and evaluated its performance on commodity cloud (i.e., amazon ec2) under various settings. experimental results demonstrated that porla achieves two to four orders of magnitude smaller audit proof size with 4\u00d7 \u2013 1,800\u00d7 lower audit time than all prior schemes in both private and public audit settings at the cost of only 2\u00d7 \u2013 3\u00d7 slower update.", "evasion attacks and defenses on smart home physical event verification": "in smart homes, when an actuator's state changes, it sends an event notification to the iot hub to report this change (e.g., the door is unlocked). prior works have shown that event notifications are vulnerable to spoofing and masking attacks. in event spoofing, an adversary reports to the iot hub a fake event notification that did not physically occur. in event masking, an adversary suppresses the notification of an event that physically occurred. these attacks create inconsistencies between physical and cyber states of actuators, enabling an adversary to indirectly gain control over safety-critical devices by triggering iot apps. to mitigate these attacks, event verification systems (evs), or broadly iot anomaly detection systems, leverage physical event fingerprints that describe the relations between events and their influence on sensor readings. however, smart homes have complex physical interactions between events and sensors that characterize the event fingerprints. our study of the recent evs, unfortunately, has revealed that they widely ignore such interactions, which enables an adversary to evade these systems and launch successful event spoofing and masking attacks without getting detected.\nin this paper, we first explore the evadable physical event fingerprints and show that an adversary can realize them to bypass the evs given the same threat model. we develop two defenses, evs software patching and sensor placement with the interplay of physical modeling and formal analysis, to generate robust physical event fingerprints and demonstrate how they can be integrated into the evs. we evaluate the effectiveness of our approach in two smart home settings that contain 12 actuators and 16 sensors when two different state-of-the-art evs are deployed. our experiments demonstrate that 71% of their physical fingerprints are vulnerable to evasion. by incorporating our approach, they build robust physical event fingerprints, and thus, properly mitigate realistic attack vectors.", "extrapolating formal analysis to uncover attacks in bluetooth passkey entry pairing": "bluetooth is a leading wireless communication technology used by billions of internet of things (iot) devices today. its ubiquity demands systematic security scrutiny. a key ingredient in bluetooth security is secure pairing, which includes numeric comparison (nc) and passkey entry (pe). however, most prior formal efforts have considered only nc, and pe has not yet been formally studied in depth. in this paper, we propose a detailed formal analysis of the pe protocol. in particular, we present a generic formal model, built using tamarin, to verify the security of pe by precisely capturing the protocol behaviors and attacker capabilities. encouragingly, it rediscovers three known attacks (confusion attacks, static passcode attacks, and reflection attacks), and more importantly, also uncovers two new attacks (group guessing attacks and ghost attacks) spanning across diverse attack vectors (e.g., static variable reuse, multi-threading, reflection, human error, and compromise device). finally, after applying fixes to each vulnerability, our model further proves the confidentiality and authentication properties of the pe protocol using an inductive base model.", "faster secure comparisons with offline phase for efficient private set intersection": "in a private section intersection (psi) protocol, alice and bob compute the intersection of their respective sets without disclosing any element not in the intersection. psi protocols have been extensively studied in the literature and are deployed in industry. with state-of-the-art protocols achieving optimal asymptotic complexity, performance improvements are rare and can only improve complexity constants. in this paper, we present a new private, extremely efficient comparison protocol that leads to a psi protocol with low constants. a useful property of our comparison protocol is that it can be divided into an online and an offline phase. all expensive cryptographic operations are performed during the offline phase, and the online phase performs only four fast field operations per comparison. this leads to an incredibly fast online phase, and our evaluation shows that it outperforms related work, including kkrt (ccs'16), vole-psi (eurocrypt'21), and  okvs (crypto'21). we also evaluate standard approaches to implement the offline phase using different trust assumptions: cryptographic, hardware, and a third party (\"dealer model\").", "fusion: efficient and secure inference resilient to malicious servers": "in secure machine learning inference, most of the schemes assume that the server is semi-honest (honestly following the protocol but attempting to infer additional information). however, the server may be malicious (e.g., using a low-quality model or deviating from the protocol) in the real world. although a few studies have considered a malicious server that deviates from the protocol, they ignore the verification of model accuracy (where the malicious server uses a low-quality model) meanwhile preserving the privacy of both the server's model and the client's inputs. to address these issues, we propose textit{fusion}, where the client mixes the public samples (which have known query results) with their own samples to be queried as the inputs of multi-party computation to jointly perform the secure inference. since a server that uses a low-quality model or deviates from the protocol can only produce results that can be easily identified by the client, textit{fusion} forces the server to behave honestly, thereby addressing all those aforementioned issues without leveraging expensive cryptographic techniques. our evaluation indicates that textit{fusion} is 48.06$times$ faster and uses 30.90$times$ less communication than the existing maliciously secure inference protocol (which currently does not support the verification of the model accuracy). in addition, to show the scalability, we conduct imagenet-scale inference on the practical resnet50 model and it costs 8.678 minutes and 10.117 gib of communication in a wan setting, which is 1.18$times$ faster and has 2.64$times$ less communication than those of the semi-honest protocol.", "ghost domain reloaded: vulnerable links in domain name delegation and revocation": "in this paper, we propose textit{phoenix domain}, a general and novel attack that allows adversaries to maintain the revoked malicious domain continuously resolvable at scale, which enables an old, mitigated attack, ghost domain. textit{phoenix domain} has two variations and affects all mainstream dns software and public dns resolvers overall because it does not violate any dns specifications and best security practices. the attack is made possible through systematically ''textit{reverse engineer}'' the cache operations of 8 dns implementations, and new attack surfaces are revealed in the domain name delegation processes. we select 41 well-known public dns resolvers and prove that all surveyed dns services are vulnerable to textit{phoenix domain}, including google public dns and cloudflare dns. extensive measurement studies are performed with 210k stable and distributed dns recursive resolvers, and results show that even after one month from domain name revocation and cache expiration, more than 25% of recursive resolvers can still resolve it. the proposed attack provides an opportunity for adversaries to evade the security practices of malicious domain take-down. we have reported discovered vulnerabilities to all affected vendors and suggested 6 types of mitigation approaches to them. until now, 7 dns software providers and 15 resolver vendors, including bind, unbound, google, and cloudflare, have confirmed the vulnerabilities, and some of them are implementing and publishing mitigation patches according to our suggestions. in addition, 9 cve numbers have been assigned. the study calls for standardization to address the issue of how to revoke domain names securely and maintain cache consistency.", "hope of delivery: extracting user locations from mobile instant messengers": "mobile instant messengers such as whatsapp use delivery status notifications in order to inform users if a sent message has successfully reached its destination. this is useful and important information for the sender due to the often asynchronous use of the messenger service. however, as we demonstrate in this paper, this standard feature opens up a timing side channel with unexpected consequences for user location privacy. we investigate this threat conceptually and experimentally for three widely spread instant messengers. we validate that this information leak even exists in privacy-friendly messengers such as signal and threema.\nour results show that, after a training phase, a messenger user can distinguish different locations of the message receiver. our analyses involving multiple rounds of measurements and evaluations show that the timing side channel persists independent of distances between receiver locations -- the attack works both for receivers in different countries as well as at small scale in one city. for instance, out of three locations within the same city, the sender can determine the correct one with more than 80% accuracy. thus, messenger users can secretly spy on each others' whereabouts when sending instant messages. as our countermeasure evaluation shows, messenger providers could effectively disable the timing side channel by randomly delaying delivery confirmations within the range of a few seconds. for users themselves, the threat is harder to prevent since there is no option to turn off delivery confirmations.", "let me unwind that for you: exceptions to backward-edge protection": "backward-edge control-flow hijacking via stack buffer overflow is the holy grail of software exploitation. the ability to directly control critical stack data and the hijacked target makes this exploitation strategy particularly appealing for attackers. as a result, the community has deployed strong backward-edge protections such as shadow stacks or stack canaries, forcing attackers to resort to less ideal e.g., heap-based exploitation strategies. however, such mitigations commonly rely on one key assumption, namely an attacker relying on return address corruption to directly hijack control flow upon function return.\nin this paper, we present *exceptions* to this assumption and show attacks based on backward-edge control-flow hijacking *without* the direct hijacking are possible. specifically, we demonstrate that stack corruption can cause exception handling to act as a *confused deputy* and mount backward-edge control-flow hijacking attacks on the attacker\u2019s behalf. this strategy\nprovides overlooked opportunities to divert execution to attacker-controlled catch handlers (a paradigm we term catch handler oriented programming or chop) and craft powerful primitives\nsuch as arbitrary code execution or arbitrary memory writes. we find chop-style attacks to work across multiple platforms (linux, windows, macos, android and ios). to analyze the uncovered attack surface, we survey popular open-source packages and study the applicability of the proposed exploitation techniques. our analysis shows that suitable exception handling\ntargets are ubiquitous in c++ programs and exploitable exception handlers are common. we conclude by presenting three end-to-end exploits on real-world software and proposing changes to deployed mitigations to address chop.", "machine unlearning of features and labels": "removing information from a machine learning model is a non-trivial task that requires to partially revert the training process. this task is unavoidable when sensitive data, such as credit card numbers or passwords, accidentally enter the model and need to be removed afterwards. recently, different concepts for machine unlearning have been proposed to address this problem. while these approaches are effective in removing individual data points, they do not scale to scenarios where larger groups of features and labels need to be reverted.\nin this paper, we propose the first method for unlearning features and labels. our approach builds on the concept of influence functions and realizes unlearning through closed-form updates of\nmodel parameters. it enables to adapt the influence of training data on a learning model retrospectively, thereby correcting data leaks and privacy issues. for learning models with strongly convex loss functions, our method provides certified unlearning with theoretical guarantees. for models with non-convex losses, we empirically show that unlearning features and labels is effective and significantly faster than other strategies.", "mytee: own the trusted execution environment on embedded devices": "we propose a solution, mytee, that enables a trusted execution environment (tee) to be built even in worst-case environments wherein major hardware security primitives (e.g., arm trustzone extensions for memory access control) are absent. crafting page tables for memory isolation, filtering dma packets, and enabling secure io exist at the core of mytee. particularly for secure io, we shield the io buffers and memory-mapped registers of the controllers and securely escalate the privilege of the partial code block of the device drivers to provide permission to access the protected objects. by doing so, the need to host the device driver in the tee (in whole or in part), which can potentially introduce a new attack surface, is exempted. the proof-of-concept (poc) of mytee is implemented on the raspberry pi 3 board, which does not support most of the important security primitives for building the tee. additionally, three secure io examples with the hardware tpm, framebuffer, and usb keyboard are demonstrated to show the feasibility of our approach.", "on the anonymity of peer-to-peer network anonymity schemes used by cryptocurrencies": "cryptocurrency systems can be subject to deanonymization attacks by exploiting the network-level communication on their peer-to-peer network. adversaries who control a set of colluding node(s) within the peer-to-peer network can observe transactions being exchanged and infer the parties involved. thus, various network anonymity schemes have been proposed to mitigate this problem, with some solutions providing theoretical anonymity guarantees.\nin this work, we model such peer-to-peer network anonymity solutions and evaluate their anonymity guarantees. to do so, we propose a novel framework that uses bayesian inference to obtain the probability distributions linking transactions to their possible originators. we characterize transaction anonymity with those distributions, using entropy as metric of adversarial uncertainty on the originator's identity. in particular, we model dandelion, dandelion++, and lightning network.\nwe study different configurations and demonstrate that none of them offers acceptable anonymity to their users. for instance, our analysis reveals that in the widely deployed lightning network, with $1%$ strategically chosen colluding nodes the adversary can uniquely determine the originator for $approx50%$ of the total transactions in the network. in dandelion, an adversary that controls $15%$ of the nodes has on average uncertainty among only $8$ possible originators. moreover, we observe that due to the way dandelion and dandelion++ are designed, increasing the network size does not correspond to an increase in the anonymity set of potential originators. alarmingly, our longitudinal analysis of lightning network reveals rather an inverse trend---with the growth of the network the overall anonymity decreases.", "pose: practical off-chain smart contract execution": "smart contracts enable users to execute payments depending on complex program logic. ethereum is the most notable example of a blockchain that supports smart contracts leveraged for countless applications including games, auctions and financial products. unfortunately, the traditional method of running contract code on-chain is very expensive, for instance, on the ethereum platform, fees have dramatically increased, rendering the system unsuitable for complex applications. a prominent solution to address this problem is to execute code off-chain and only use the blockchain as a trust anchor. while there has been significant progress in developing off-chain systems over the last years, current off-chain solutions suffer from various drawbacks including costly blockchain interactions, lack of data privacy, huge capital costs from locked collateral, or supporting only a restricted set of applications.\nin this paper, we present pose\u2014a practical off-chain protocol for smart contracts that addresses the aforementioned shortcomings of existing solutions. pose leverages a pool of trusted execution environments (tees) to execute the computation efficiently and to swiftly recover from accidental or malicious failures. we show that pose provides strong security guarantees even if a large subset of parties is corrupted. we evaluate our proof-of-concept implementation with respect to its efficiency and effectiveness.", "post-gdpr threat hunting on android phones: dissecting os-level safeguards of user-unresettable identifiers": "ever since its genesis, android has enabled apps to access data and services on mobile devices. this however involves a wide variety of user-unresettable identifiers (uuis), e.g., the mac address, which are associated with a device permanently. given their privacy sensitivity, android has tightened its uui access policy since its version 10, in response to the increasingly strict privacy protection regulations around the world. non-system apps are restricted from accessing them and are required to use user-resettable alternatives such as advertising ids.\nin this work, we conduct a systematic study on the effectiveness of the uui safeguards on android phones including both android open source project (aosp) and original equipment manufacturer (oem) phones. to facilitate our large-scale study, we propose a set of analysis techniques that discover and assess uui access channels. our approach features a hybrid analysis that consists of static program analysis of android framework and forensic analysis of os images to uncover access channels. these channels are then tested with differential analysis to identify weaknesses that open any attacking opportunity. we have conducted a vulnerability assessment on 13 popular phones of 9 major manufacturers, most of which are top-selling and installed with the recent android versions. our study reveals that uui mishandling pervasively exists, evidenced by 51 unique vulnerabilities found (8 listed by cve). our work unveils the status quo of the uui protection in android phones, complementing the existing studies that mainly focus on apps' uui harvesting behaviors. our findings should raise an alert to phone manufacturers and would encourage policymakers to further extend the scope of regulations with device-level data protection.", "ppa: preference profiling attack against federated learning": "federated learning (fl) trains a global model across a number of decentralized users, each with a local dataset. compared to traditional centralized learning, fl does not require direct access to local datasets and thus aims to mitigate data privacy concerns. however, data privacy leakage in fl still exists due to inference attacks, including membership inference, property inference, and data inversion.\nin this work, we propose a new type of privacy inference attack, coined preference profiling attack (ppa), that accurately profiles the private preferences of a local user, e.g., most liked (disliked) items from the client's online shopping and most common expressions from the user's selfies.  in general, ppa can profile top-$k$ (i.e., $k$ = $1, 2, 3$ and $k = 1$ in particular) preferences contingent on the local client (user)'s characteristics. our key insight is that the gradient variation of a local user's model has a distinguishable sensitivity to the sample proportion of a given class, especially the majority (minority) class. by observing a user model's gradient sensitivity to a class, ppa can profile the sample proportion of the class in the user's local dataset, and thus textit{the user's preference of the class} is exposed. the inherent statistical heterogeneity of fl further facilitates ppa. we have extensively evaluated the ppa's effectiveness using four datasets (mnist, cifar10, raf-db and products-10k). our results show that ppa achieves 90% and 98% top-$1$ attack accuracy to the mnist and cifar10, respectively. more importantly, in real-world commercial scenarios of shopping (i.e., products-10k) and social network (i.e., raf-db), ppa gains a top-$1$ attack accuracy of 78% in the former case to infer the most ordered items (i.e., as a commercial competitor), and 88% in the latter case to infer a victim user's most often facial expressions, e.g., disgusted. the top-$3$ attack accuracy and top-$2$ accuracy is up to 88% and 100% for the products-10k and raf-db, respectively. we also show that ppa is insensitive to the number of fl's local users (up to 100 we tested) and local training epochs (up to 20 we tested) used by a user. although existing countermeasures such as dropout and differential privacy protection can lower the ppa's accuracy to some extent, they unavoidably incur notable deterioration to the global model. the source code is available at https://github.com/ppaattack.", "private certifier intersection": "we initiate the study of private certifier intersection (pci), which allows mutually distrusting parties to establish a trust basis for cross-validation of claims if they have one or more trust authorities (certifiers) in common. this is one of the essential requirements for verifiable presentations in web 3.0, since it provides additional privacy without compromising on decentralization. a pci protocol allows two or more parties holding certificates to identify a common set of certifiers while additionally validating the certificates issued by such certifiers, without leaking any information about the certifiers not in the output intersection. in this paper, we formally define the notion of multi-party pci in the simplified-uc framework for two different settings depending on whether certificates are required for any of the claims (called pci-any) or all of the claims (called pci-all). we then design and implement two provably secure and practically efficient pci protocols supporting validation of digital signature-based certificates: a pci-any protocol for ecdsa-based certificates and a pci-all protocol for bls-based certificates. the technical centerpiece of our proposals is the first secretsharing-based mpc framework supporting efficient computation of elliptic curve-based arithmetic operations, including elliptic curve pairings, in a black-box way. we implement this framework by building on top of the well-known mp-spdz library using openssl and relic for elliptic curve operations, and use this implementation to benchmark our proposed pci protocols in the lan and wan settings. in an intercontinental wan setup with parties located in different continents, our protocols execute in less than a minute on input sets of size 40, which demonstrates the practicality of our proposed solutions.", "quicforge: client-side request forgery in quic": "the quic protocol is gaining more and more traction through its recent standardization and the rising interest by various big tech companies, developing new implementations. quic promises to make security and privacy a first-class citizen; yet, challenging these claims is of utmost importance. to this end, this paper provides an initial analysis of client-side request forgery attacks that directly emerge from the quic protocol design and not from common vulnerabilities. in particular, we investigate three request forgery attack modalities with respect to their capabilities to be used for protocol impersonation and traffic amplification. we analyze the controllable attack space of the respective protocol messages and demonstrate that one of the attack modalities can indeed be utilized to impersonate other udp-based protocols, e.g., dns requests. furthermore, we identify traffic amplification vectors. although the quic protocol specification states anti-amplification limits, our evaluation of 13 quic server implementations shows that in some cases these mitigations are missing or insufficiently implemented. lastly, we propose mitigation approaches for protocol impersonation and discuss ambiguities in the specification.", "rovisq: reduction of video service quality via adversarial attacks on deep learning-based video compression": "video compression plays a crucial role in video streaming and classification systems by maximizing the end-user quality of experience (qoe) at a given bandwidth budget.\nin this paper, we conduct the first systematic study for adversarial attacks on deep learning-based video compression and downstream classification systems. our attack framework, dubbed rovisq, manipulates the rate-distortion (r-d) relationship of a video compression model to achieve one or both of the following goals: (1) increasing the network bandwidth, (2) degrading the video quality for end-users. we further devise new objectives for targeted and untargeted attacks to a downstream video classification service. finally, we design an input-invariant perturbation that universally disrupts video compression and classification systems in real time. unlike previously proposed attacks on video classification, our adversarial perturbations are the first to withstand compression. \nwe empirically show the resilience of rovisq attacks against various defenses, i.e., adversarial\ntraining, video denoising, and jpeg compression. our extensive experimental results on various video datasets show rovisq attacks deteriorate peak signal-to-noise ratio by up to 5.6db and the bit-rate by up to ~ 2.4 times while achieving over 90% attack success rate on a downstream classifier. our user study further demonstrates the effect of rovisq attacks on users\u2019 qoe. we provide several example attacked videos used in our survey on https://sites.google.com/view/demo-of-rovisq/home.", "securing federated sensitive topic classification against poisoning attacks": "we present a federated learning (fl) based solution for building a distributed classifier capable of detecting urls containing sensitive content, i.e., content related to categories such as health, political beliefs, sexual orientation, etc. although such a classifier addresses the limitations of previous offline/centralised classifiers, it is still vulnerable to poisoning attacks from malicious users that may attempt to reduce the accuracy for benign users by disseminating faulty model updates. to guard against this, we develop a robust aggregation scheme based on subjective logic and residual-based attack detection. employing a combination of theoretical analysis, trace-driven simulation, as well as experimental validation with a prototype and real users, we show that our classifier can detect sensitive content with high accuracy, learn new labels fast, and remain robust in view of poisoning attacks from malicious users, as well as imperfect input from non-malicious ones.", "smarter contracts: detecting vulnerabilities in smart contracts with deep transfer learning": "ethereum smart contracts are automated decentralized applications on the blockchain that describe the terms of the agreement between buyers and sellers, reducing the need for trusted intermediaries and arbitration. however, the deployment of smart contracts introduces new attack vectors into the cryptocurrency systems. in particular, programming flaws in smart contracts have been already exploited to lead to enormous financial loss. hence, it is crucial to detect various vulnerability types in contracts effectively and efficiently. existing vulnerability detection methods are limited in scope as they typically focus on one or a very limited set of vulnerabilities. also, extending them to new vulnerability types requires costly re-design.\nin this work, we develop escort, a deep learning-based vulnerability detection method that uses a common feature extractor to learn generic bytecode semantics of smart contracts and separate branches to learn the features of each vulnerability type. as a multi-label classifier, escort can detect multiple vulnerabilities of the contract at once. compared to prior detection methods, escort can be easily extended to new vulnerability types with limited data via transfer learning. when a new vulnerability type emerges, escort adds a new branch to the trained feature extractor and trains it with limited data. we evaluated escort on a dataset of 3.61 million smart contracts and demonstrate that it achieves an average f1 score of 98% on six vulnerability types in initial training and yields an average f1 score of 96% in transfer learning phase on five additional vulnerability types. to the best of our knowledge, escort is the first deep learning-based framework that utilizes transfer learning on new vulnerability types with minimal model modification and re-training overhead. compared with existing non-ml tools, escort can be applied to contracts of arbitrary complexity and ensures 100% contract coverage. in addition, we enable concurrent detection of multiple vulnerability types using a single unified framework, thus avoiding the efforts of setting up multiple tools and greatly reducing the detection time. we will open source our dataset and the data labeling toolchain to facilitate future research.", "tactics, threats &#038; targets: modeling disinformation and its mitigation": "disinformation can be used to sway public opinion toward a certain political or economic direction, adversely impact public health, and mobilize groups to engage in violent disobedience. a major challenge in mitigation is scarcity: disinformation is widespread but its mitigators are few. in this work, we interview fact-checkers, journalists, trust and safety specialists, researchers, and analysts who work in different organizations tackling problematic information across the world. from this interview study, we develop an understanding of the reality of combating disinformation across domains, and we use our findings to derive a cybersecurity-inspired framework to characterize the threat of disinformation. while related work has developed similar frameworks for conducting analyses and assessment, our work is distinct in providing the means to thoroughly consider the attacker side, their tactics and approaches. we demonstrate the applicability of our framework on several examples of recent disinformation campaigns.", "the &#8220;beatrix&#8221; resurrections: robust backdoor detection via gram matrices": "deep neural networks (dnns) are susceptible to backdoor attacks during training. the model corrupted in this way functions normally, but when triggered by certain patterns in the input, produces a predefined target label. existing defenses usually rely on the assumption of the universal backdoor setting in which poisoned samples share the same uniform trigger. however,\nrecent advanced backdoor attacks show that this assumption is no longer valid in dynamic backdoors where the triggers vary from input to input, thereby defeating the existing defenses.\nin this work, we propose a novel technique, beatrix (backdoor detection via gram matrix). beatrix utilizes gram matrix to capture not only the feature correlations but also the appropriately high-order information of the representations. by learning class-conditional statistics from activation patterns of normal samples, beatrix can identify poisoned samples by capturing the anomalies in activation patterns. to further improve the performance in identifying target labels, beatrix leverages kernel-based testing without making any prior assumptions on representation distribution. we demonstrate the effectiveness of our method through extensive evaluation and comparison with state-of-the-art defensive techniques. the experimental results show that our approach achieves an f1 score of 91.1% in detecting dynamic backdoors, while the state of the art can only reach 36.9%.", "towards automatic and precise heap layout manipulation for general-purpose programs": "a critical challenge in automatic exploit generation is to find out whether an exploitable state can be constructed by manipulating the heap layout. this is usually achieved by re-arranging the objects in heap memory according to an orchestrated strategy that utilizes the program's heap operations. however, hindered by the difficulty in strategically coordinating the use of heap operations given the complexity in the program logic and heap allocation mechanisms,  the goal of precise heap layout manipulation for general-purpose programs has not been accomplished.\nin this paper, we present bagua, an innovative solution towards automatically and precisely manipulating heap layouts for general-purpose programs. specifically, bagua first precisely identifies the primitives of heap layout manipulation using the heap operation dependence graph and thoroughly analyzes their dependencies and capabilities. on this basis, it models the heap layout manipulation as an integer linear programming problem and solves the constraints, in order to identify the sequence of primitives that achieves a desired heap layout. by triggering the primitives in such an order, we are able to construct new proof-of-concept inputs of target programs to achieve an exploitable heap layout. highlights of our research include a set of new techniques that address the specific challenges of analyzing general-purpose programs, such as eliminating the side effect of heap allocators and extending the capability in manipulating heap layouts. we implemented a prototype of bagua and evaluated it on 27 publicly-known bugs in real-world programs. with bagua's strength in pinpointing primitives and handling the side effect of heap allocators, it successfully generates desired heap layouts for 23 of the bugs, which is way beyond what prior research can achieve.", "trellis: robust and scalable metadata-private anonymous broadcast": "trellis is a mix-net based anonymous broadcast\nsystem with cryptographic security guarantees. trellis can be used\nto anonymously publish documents or communicate with other\nusers, all while assuming full network surveillance. in trellis,\nusers send messages through a set of servers in successive rounds.\nthe servers mix and post the messages to a public bulletin board,\nhiding which users sent which messages.\ntrellis hides all network-level metadata, remains robust to\nchanging network conditions, guarantees availability to honest\nusers, and scales with the number of mix servers. trellis provides three to five orders of magnitude faster performance and\nbetter network robustness compared to atom, the state-of-the-art\nanonymous broadcast system with a similar threat model.\nin achieving these guarantees, trellis contributes: (1) a\nsimpler theoretical mixing analysis for a routing mix network\nconstructed with a fraction of malicious servers, (2) anonymous\nrouting tokens for verifiable random paths, and (3) lightweight\nblame protocols built on top of onion routing to identify and\neliminate malicious parties.\nwe implement and evaluate trellis in a networked deployment. with 64 servers located across four geographic regions,\ntrellis achieves a throughput of 220 bits per second with 100,000\nusers. with 128 servers, trellis achieves a throughput of 320\nbits per second. trellis\u2019s throughput is only 100 to 1000\u00d7 slower\ncompared to tor (which has 6,000 servers and 2m daily users)\nand is therefore potentially deployable at a smaller \u201centerprise\u201d\nscale. our implementation is open-source.", "viceroy: gdpr-/ccpa-compliant enforcement of verifiable accountless consumer requests": "recent data protection regulations (notably, gdpr and ccpa) grant consumers various rights, including the right to access, modify or delete any personal information collected about them (and retained) by a service provider. to exercise these rights, one must submit a verifiable consumer request proving that the collected data indeed pertains to them. this action is straightforward for consumers with active accounts with a service provider at the time of data collection, since they can use standard (e.g., password-based) means of authentication to validate their requests. however, a major conundrum arises from the need to support consumers without accounts to exercise their rights. to this end, some service providers began requiring such accountless consumers to reveal and prove their identities (e.g., using government-issued documents, utility bills, or credit card numbers) as part of issuing a verifiable consumer request. while understandable as a short-term fix, this approach is cumbersome and expensive for service providers as well as privacy-invasive for consumers. \nconsequently, there is a strong need to provide better means of authenticating requests from accountless consumers. to achieve this, we propose viceroy, a privacy-preserving and scalable framework for producing proofs of data ownership, which form a basis for verifiable consumer requests. building upon existing web techniques and features, viceroy allows accountless consumers to interact with service providers, and later prove that they are the same person in a privacy-preserving manner, while requiring minimal changes for both parties. we design and implement viceroy with emphasis on security/privacy, deployability and usability. we also assess its practicality via extensive experiments.", "your router is my prober: measuring ipv6 networks via icmp rate limiting side channels": "active internet measurements face challenges when some measurements require many remote vantage points. in this paper, we propose a novel technique for measuring remote ipv6 networks via side channels in icmp rate limiting, a required function for ipv6 nodes to limit the rate at which icmp error messages are generated. this technique, *ivantage*, can to some extent use 1.1m remote routers distributed in 9.5k autonomous systems and 182 countries as our \u201cvantage points\u201d.we apply *ivantage* to two different, but both challenging measurement tasks: 1) measuring the deployment of inbound source address validation (isav) and 2) measuring reachability between arbitrary internet nodes. we accomplish these two tasks from only one local vantage point without controlling the targets or relying on other services within the target networks. our large-scale isav measurements cover ~50% of all ipv6 autonomous systems and find ~79% of them are vulnerable to spoofing, which is the most large-scale measurement study of ipv6 isav to date. our method for reachability measurements achieves over 80% precision and recall in our evaluation. finally, we perform an internet-wide measurement of the icmp rate limiting implementations, present a detailed discussion on icmp rate limiting, particularly the potential security and privacy risks in the mechanism of icmp rate limiting, and provide possible mitigation measures. we make our code available to the community.", "a security study about electron applications and a programming methodology to tame dom functionalities": "the electron platform represents a paradigm to develop modern desktop apps using html and javascript. microsoft teams, visual studio code and other flagship products are examples of electron apps. this new paradigm inherits the security challenges in web programming into the desktop-app realm, thus opens a new way for local-machine exploitation. we conducted a security study about real-world electron apps, and discovered many vulnerabilities that are now confirmed by the app vendors. the conventional wisdom is to view these bugs as *sanitization errors*. accordingly, secure programming requires programmers to explicitly enumerate all kinds of unexpected inputs to sanitize. we believe that secure programming should focus on specifying programmers' intentions as opposed to their non-intentions. we introduce a concept called *dom-tree type*, which expresses the set of dom trees that an app expects to see during execution, so an exploit will be caught as a type violation. with insights into the html standard and the chromium engine, we build the dom-tree type mechanism into the electron platform. the evaluations show that the methodology is practical, and it secures all vulnerable apps that we found in the study.", "access your tesla without your awareness: compromising keyless entry system of model 3": "tesla model 3 has equipped with phone keys and key cards in addition to traditional key fobs for better driving experiences. these new features allow a driver to enter and start the vehicle without using a mechanical key through a wireless authentication process between the vehicle and the key. unlike the requirements of swiping against the car for key cards, the tesla mobile app\u2019s phone key feature can unlock a model 3 while your smartphone is still in a pocket or bag. \nin this paper, we performed a detailed security analysis aiming at tesla keys, especially for key cards and phone keys. starting with reverse engineering the mobile application and sniffing the communication data, we reestablished pairing and authentication protocols and analyzed their potential issues. missing the certificate verification allows an unofficial key card to work as an official one. using these third-party products may lead to serious security problems. also, the weaknesses of the current protocol lead to a man-in-the-middle (mitm) attack through a bluetooth channel. the mitm attack is an improved relay attack breaking the security of the authentication procedures for phone keys. we also developed an app named tesmla installed on customized android devices to complete the proof-of-concept. the attackers can break into tesla model 3 and drive it away without the awareness of the car owner. our results bring into question the security of passive keyless entry and start (pkes) and bluetooth implementations in security-critical applications. to mitigate the security problems, we discussed the corresponding countermeasures and feasible secure scheme in the future.", "accountable javascript code delivery": "the internet is a major distribution platform for web applications, but there are\nno effective transparency and audit mechanisms in place for the web. due to the ephemeral nature\nof web applications, a client visiting a website has no guarantee that the\ncode it receives today is the same as yesterday, or the same as other visitors\nreceive. despite advances in web security, it is thus challenging to audit\nweb applications before they are rendered in the browser. we propose\naccountable js, a browser extension and opt-in protocol for accountable\ndelivery of active content on a web page. we prototype our protocol,\nformally model its security properties with the tamarin prover, and\nevaluate its compatibility and performance impact with case studies\nincluding whatsapp web, adsense and nimiq. \naccountability is beginning to be deployed at scale, with meta\u2019s recent announcement of code verify\navailable to all 2 billion whatsapp users, but there has been little formal analysis of such protocols.\nwe formally model code verify using the tamarin prover and compare its properties to our accountable js protocol. we also compare code verify\u2019s and accountable js extension's performance impacts on whatsapp web.", "adversarial robustness for tabular data through cost and utility awareness": "many safety-critical applications of machine learning, such as fraud or abuse detection, use data in tabular domains. adversarial examples can be particularly damaging for these applications. yet, existing works on adversarial robustness primarily focus on machine-learning models in image and text domains. we argue that, due to the differences between tabular data and images or text, existing threat models are not suitable for tabular domains. these models do not capture that the costs of an attack could be more significant than imperceptibility, or that the adversary could assign different values to the utility obtained from deploying different adversarial examples. we demonstrate that, due to these differences, the attack and defense methods used for images and text cannot be directly applied to tabular settings. we address these issues by proposing new cost and utility-aware threat models that are tailored to the adversarial capabilities and constraints of attackers targeting tabular domains. we introduce a framework that enables us to design attack and defense mechanisms that result in models protected against cost or utility-aware adversaries, for example, adversaries constrained by a certain financial budget. we show that our approach is effective on three datasets corresponding to applications for which adversarial examples can have economic and social implications.", "anomaly detection in the open world: normality shift detection, explanation, and adaptation": "concept drift is one of the most frustrating challenges for learning-based security applications built on the close-world assumption of identical distribution between training and deployment. anomaly detection, one of the most important tasks in security domains, is instead immune to the drift of abnormal behavior due to the training without any abnormal data (known as zero-positive), which however comes at the cost of more severe impacts when normality shifts. however, existing studies mainly focus on concept drift of abnormal behaviour and/or supervised learning, leaving the normality shift for zero-positive anomaly detection largely unexplored.\nin this work, we are the first to explore the normality shift for deep learning-based anomaly detection in security applications, and propose owad, a general framework to detect, explain and adapt to normality shift in practice. in particular, owad outperforms prior work by detecting shift in an unsupervised fashion, reducing the overhead of manual labeling, and providing better adaptation performance through distribution-level tackling. we demonstrate the effectiveness of owad through several realistic experiments on three security-related anomaly detection applications with long-term practical data. results show that owad can provide better adaptation performance of normality shift with less labeling overhead. we provide case studies to analyze the normality shift and provide operational recommendations for security applications. we also conduct an initial real-world deployment on a scada security system.", "assessing the impact of interface vulnerabilities in compartmentalized software": "least-privilege separation decomposes applications into compartments limited to accessing only what they need. when compartmentalizing existing software, many approaches neglect securing the new inter-compartment interfaces, although what used to be a function call from/to a trusted component is now potentially a targeted attack from a malicious compartment. this results in an entire class of security bugs: compartment interface vulnerabilities (civs).\nthis paper provides an in-depth study of civs. we taxonomize these issues and show that they affect all known compartmentalization approaches. we propose conffuzz, an in-memory fuzzer specialized to detect civs at possible compartment boundaries. we apply conffuzz to a set of 25 popular applications and 36 possible compartment apis, to uncover a wide data-set of 629 vulnerabilities. we systematically study these issues, and extract numerous insights on the prevalence of civs, their causes, impact, and the complexity to address them. we stress the critical importance of civs in compartmentalization approaches, demonstrating an attack to extract isolated keys in openssl and uncovering a decade-old vulnerability in sudo. we show, among others, that not all interfaces are affected in the same way, that api size is uncorrelated with civ prevalence, and that addressing interface vulnerabilities goes beyond writing simple checks. we conclude the paper with guidelines for civ-aware compartment interface design, and appeal for more research towards systematic civ detection and mitigation.", "attacks as defenses: designing robust audio captchas using attacks on automatic speech recognition systems": "audio captchas are supposed to provide a strong defense for online resources; however, advances in speech-to-text mechanisms have rendered these defenses ineffective.  audio captchas cannot simply be abandoned, as they are specifically named by the w3c as important enablers of accessibility.  accordingly, demonstrably more robust audio captchas are important to the future of a secure and accessible web.  we look to recent literature on attacks on speech-to-text systems for inspiration for the construction of robust, principle-driven audio defenses.  we begin by comparing 20 recent attack papers, classifying and measuring their suitability to serve as the basis of new \"robust to transcription\" but \"easy for humans to understand\" captchas. after showing that none of these attacks alone are sufficient, we propose a new mechanism that is both comparatively intelligible (evaluated through a user study) and hard to automatically transcribe (i.e., $p({rm transcription}) = 4 times 10^{-5}$). we also demonstrate that our audio samples have a high probability of being detected as captchas when given to speech-to-text systems ($p({rm evasion}) = 1.77 times 10^{-4}$). finally, we show that our method is robust to waveguard, a popular mechanism designed to defeat adversarial examples (and enable asrs to output the original transcript instead of the adversarial one). we show that our method can break waveguard with a 99% success rate. in so doing, we not only demonstrate a captcha that is approximately four orders of magnitude more difficult to crack, but that such systems can be designed based on the insights gained from attack papers using the differences between the ways that humans and computers process audio.", "backdoor attacks against dataset distillation": "dataset distillation has emerged as a prominent technique to improve data efficiency when training machine learning models. it encapsulates the knowledge from a large dataset into a smaller synthetic dataset. a model trained on this smaller distilled dataset can attain comparable performance to a model trained on the original training dataset. however, the existing dataset distillation techniques mainly aim at achieving the best trade-off between resource usage efficiency and model utility. the security risks stemming from them have not been explored. this study performs the first backdoor attack against the models trained on the data distilled by dataset distillation models in the image domain. concretely, we inject triggers into the synthetic data during the distillation procedure rather than during the model training stage, where all previous attacks are performed. we propose two types of backdoor attacks, namely naiveattack and doorping. naiveattack simply adds triggers to the raw data at the initial distillation phase, while doorping iteratively updates the triggers during the entire distillation procedure. we conduct extensive evaluations on multiple datasets, architectures, and dataset distillation techniques. empirical evaluation shows that naiveattack achieves decent attack success rate (asr) scores in some cases, while doorping reaches higher asr scores (close to 1.0) in all cases. furthermore, we conduct a comprehensive ablation study to analyze the factors that may affect the attack performance. finally, we evaluate multiple defense mechanisms against our backdoor attacks and show that our attacks can practically circumvent these defense mechanisms.", "bars: local robustness certification for deep learning based traffic analysis systems": "deep learning (dl) performs well in many traffic analysis tasks. nevertheless, the vulnerability of deep learning weakens the real-world performance of these traffic analyzers (e.g., suffering from evasion attack). many studies in recent years focused on robustness certification for dl-based models. but existing methods perform far from perfectly in the traffic analysis domain. in this paper, we try to match three attributes of dl-based traffic analysis systems at the same time: (1) highly heterogeneous features, (2) varied model designs, (3) adversarial operating environments. therefore, we propose bars, a general robustness certification framework for dl-based traffic analysis systems based on boundary-adaptive randomized smoothing. to obtain tighter robustness guarantee, bars uses optimized smoothing noise converging on the classification boundary. we firstly propose the distribution transformer for generating optimized smoothing noise. then to optimize the smoothing noise, we propose some special distribution functions and two gradient based searching algorithms for noise shape and noise scale. we implement and evaluate bars in three practical dl-based traffic analysis systems. experiment results show that bars can achieve tighter robustness guarantee than baseline methods. furthermore, we illustrate the practicability of bars through five application cases (e.g., quantitatively evaluating robustness).", "beagle: forensics of deep learning backdoor attack for better defense": "deep learning backdoor attacks have a threat model similar to traditional cyber attacks. attack forensics, a critical counter-measure for traditional cyber attacks, is hence of importance for defending model backdoor attacks. in this paper, we propose a novel model backdoor forensics technique. given a few attack samples such as inputs with backdoor triggers, which may represent different types of backdoors, our technique automatically decomposes them to clean inputs and the corresponding triggers. it then clusters the triggers based on their properties to allow automatic attack categorization and summarization. backdoor scanners can then be automatically synthesized to find other instances of the same type of backdoor in other models. our evaluation on 2,532 pre-trained models, 10 popular attacks, and comparison with 9 baselines show that our technique is highly effective. the decomposed clean inputs and triggers closely resemble the ground truth. the synthesized scanners substantially outperform the vanilla versions of existing scanners that can hardly generalize to different kinds of attacks.", "blockscope: detecting and investigating propagated vulnerabilities in forked blockchain projects": "due to the open-source nature of the blockchain ecosystem, it is common for new blockchains to fork or partially reuse the code of classic blockchains. for example, the popular dogecoin, litecoin, binance bsc, and polygon are all variants of bitcoin/ethereum. these \u201cforked\u201d blockchains thus could encounter similar vulnerabilities that are propagated from bitcoin/ethereum during forking or subsequently commit fetching. in this paper, we conduct a systematic study of detecting and investigating the propagated vulnerabilities in forked blockchain projects. to facilitate this study, we propose blockscope, a novel tool that can effectively and efficiently detect multiple types of cloned vulnerabilities given an input of existing bitcoin/ethereum security patches. specifically, blockscope adopts similarity-based code match and designs a new way of calculating code similarity to cover all the syntax-wide variant (i.e., type-1, type-2, and type-3) clones. moreover, blockscope automatically extracts and leverages the contexts of patch code to narrow down the search scope and locate only potentially relevant code for comparison.\nour evaluation shows that blockscope achieves good precision and high recall both at 91.8% (1.8 times higher recall than that in the state-of-the-art redebug while with close precision). blockscope allows us to discover 101 previously unknown vulnerabilities in 13 out of the 16 forked projects of bitcoin and ethereum, including 16 from dogecoin, 6 from litecoin, 1 from binance bsc, and 4 from optimism. we have reported all the vulnerabilities to their developers; 40 of them have been patched or accepted, 66 were acknowledged or under pending, and only 4 were rejected. we further investigate the propagation and patching processes of discovered vulnerabilities, and reveal three types of vulnerability propagation from source to forked projects, as well as the long delay (mostly over 200 days) for releasing patches in bitcoin forks (vs. \u223c100 days for ethereum forks).", "breaking and fixing virtual channels: domino attack and donner": "payment channel networks (pcns) mitigate the scalability issues of current decentralized cryptocurrencies. they allow for arbitrarily many payments between users connected through a path of intermediate payment channels, while requiring interacting with the blockchain only to open and close the channels. unfortunately, pcns are (i) tailored to payments, excluding more complex smart contract functionalities, such as the oracle-enabling discreet log contracts and (ii) their need for active participation from intermediaries may make payments unreliable, slower, expensive, and privacy-invasive. virtual channels are among the most promising techniques to mitigate these issues, allowing two endpoints of a path to create a direct channel over the intermediaries without any interaction with the blockchain. after such a virtual channel is constructed, (i) the endpoints can use this direct channel for applications other than payments and (ii) the intermediaries are no longer involved in updates.\nin this work, we first introduce the domino attack, a new dos/griefing style attack that leverages virtual channels to destruct the pcn itself and is inherent to the design adopted by the existing bitcoin-compatible virtual channels. we then demonstrate its severity by a quantitative analysis on a snapshot of the lightning network (ln), the most widely deployed pcn at present. we finally discuss other serious drawbacks of existing virtual channel designs, such as the support for only a single intermediary, a latency and blockchain overhead linear in the path length, or a non-constant storage overhead per user.\nwe then present donner, the first virtual channel construction that overcomes the shortcomings above, by relying on a novel design paradigm. we formally define and prove security and privacy properties in the universal composability framework. our evaluation shows that donner is efficient, reduces the on-chain number of transactions for disputes from linear in the path length to a single one, which is the key to prevent domino attacks, and reduces the storage overhead from logarithmic in the path length to constant. donner is bitcoin-compatible and can be easily integrated in the ln.", "chkplug: checking gdpr compliance of wordpress plugins via cross-language code property graph": "wordpress, a well-known content management system (cms), provides so-called plugins to augment default functionalities. one challenging problem of deploying wordpress plugins is that they may collect and process user data, such as personal identifiable information (pii), which is usually regulated by laws such as general data protection regulation (gdpr). to the best of our knowledge, no prior works have studied gdpr compliance in wordpress plugins, which often involve multiple program languages, such as php, javascript, html, and sql. \nin this paper, we design chkplug, the first automated gdpr checker of wordpress plugins for their compliance with gdpr articles related to pii. the key to chkplug is to match wordpress plugin behavior with gdpr articles using graph queries to a novel cross-language code property graph (ccpg). specifically, the ccpg models both inline language integration (such as php and html) and key-value-related connection (such as html and javascript). chkplug reports a gdpr violation if certain patterns are found in the ccpg. \nwe evaluated chkplug with human-annotated wordpress plugins.  our evaluation shows that chkplug achieves good performance with 98.8% tnr (true negative rate) and 89.3% tpr (true positive rate) in checking whether a certain wordpress plugin complies with gdpr. to investigate the current surface of the marketplace, we perform a measurement analysis which shows that 368 plugins violate data deletion regulations, meaning plugins do not provide any functionalities to erase user information from the website.", "copy-on-flip: hardening ecc memory against rowhammer attacks": "despite nearly decade-long mitigation efforts in academia and industry, the\ncommunity is yet to find a practical solution to the rowhammer vulnerability.\ncomprehensive software mitigations require complex changes to commodity systems, yielding significant run-time overhead and deterring practical\nadoption. hardware mitigations, on the other hand, have generally grown more robust and efficient, but are difficult to deploy on commodity systems. until recently, ecc memory implemented by the memory controller on server platforms seemed to provide the best of both worlds: use hardware features already on commodity systems to efficiently turn rowhammer into a denial-of-service attack vector. unfortunately, researchers have recently shown that attackers can perform one-bit-at-a-time memory templating and mount ecc-aware rowhammer attacks.\nin this paper, we reconsider ecc memory as an avenue for rowhammer mitigations\nand show that not all hope is lost. in particular, we show that it is feasible to devise a software-based design to both efficiently and effectively harden commodity ecc memory against ecc-aware rowhammer attacks. to support this claim, we present copy-on-flip (cof), an ecc-based software mitigation which uses a combination of memory _migration_ and _offlining_ to stop rowhammer attacks on commodity server systems in a practical way. the key idea is to let the operating system interpose on all the error correction events and offline the vulnerable victim page as soon as the attacker has successfully templated a sufficient number of bit flips---while transparently migrating the victim data to a new page. we present a cof prototype on linux, where we also show it is feasible to operate simple memory management changes to support migration for the relevant user and kernel memory pages. our evaluation shows cof incurs low performance and memory overhead, while significantly reducing the rowhammer attack surface. on typical benchmarks such as spec cpu2017 and google chrome, cof reports a $<1.5%$ overhead, and, on extreme i/o-intensive scenarios (saturated nginx), up to $sim11%$.", "cryptographic oracle-based conditional payments": "we consider a scenario where two mutually distrustful parties, alice and bob, want to perform a payment conditioned on the outcome of some real-world event. a semi-trusted oracle (or a threshold number of oracles, in a distributed trust setting) is entrusted to attest that such an outcome indeed occurred, and only then the payment is successfully made. such oracle-based conditional (obc) payments are ubiquitous in many real-world applications, like financial adjudication, pre-scheduled payments or trading, and are a necessary building block to introduce information about real-world events into blockchains. in this work we show how to realize obc payments with provable security guarantees and efficient instantiations. to do this, we propose a new cryptographic primitive that we call verifiable witness encryption based on threshold signatures (vwets): users can encrypt signatures on payments that can be decrypted if a threshold number of signers (e.g., oracles) sign another message (e.g., the description of an event outcome). we require two security notions: (1) one-wayness that guarantees that without the threshold number of signatures, the ciphertext hides the encrypted signature, and (2) verifiability, that guarantees that a ciphertext that correctly verifies can be successfully decrypted\nto reveal the underlying signature. we present provably secure and efficient instantiations of vwets where the encrypted signature can be some of the widely used schemes like schnorr, ecdsa or bls signatures. our main technical innovation is a new batching technique for cut-and- choose, inspired by the work of lindell-riva on garbled circuits. our vwets instantiations can be readily used to realize obc payments on virtually all cryptocurrencies of today in a fungible, cost-efficient, and scalable manner. the resulting obc payments are the first to support distributed trust (i.e., multiple oracles) without requiring any form of synchrony or coordination among the users and the oracles. to demonstrate the practicality of our scheme, we present a prototype implementation and our benchmarks in commodity hardware show that the computation overhead is less than 25 seconds even for a threshold of 4-of-7 and a payment conditioned on 1024 different real-world event outcomes, while the communication overhead is below 2.3 mb", "diffcsp: finding browser bugs in content security policy enforcement through differential testing": "the content security policy (csp) is one of the de facto security mechanisms that mitigate web threats. many websites have been deploying csps mainly to mitigate cross-site scripting (xss) attacks by instructing client browsers to constrain javascript (js) execution. however, a browser bug in csp enforcement enables an adversary to bypass a deployed csp, posing a security threat. as the csp specification evolves, csp becomes more complicated in supporting an increasing number of directives, which brings additional complexity to implementing correct enforcement behaviors. unfortunately, the finding of csp enforcement bugs in a systematic way has been largely understudied. \nin this paper, we propose diffcsp, the first differential testing framework to find csp enforcement bugs involving js execution. diffcsp generates csps and a comprehensive set of html instances that exhibit all known ways of executing js snippets. diffcsp then executes each html instance for each generated policy across different browsers, thereby collecting inconsistent execution results. to analyze a large volume of the execution results, we leverage a decision tree and identify common causes of the observed inconsistencies. we demonstrate the efficacy of diffcsp by finding 29 security bugs and eight functional bugs. we also show that three bugs are due to unclear descriptions of the csp specification. we further identify the common root causes of csp enforcement bugs, such as incorrect csp inheritance and hash handling. we confirm the risky trend of client browsers deriving completely different interpretations from the same csps, which raises security concerns. our study demonstrates the effectiveness of diffcsp for identifying csp enforcement bugs, and our findings have contributed to patching 12 security bugs in major browsers, including chrome and safari.", "do not give a dog bread every time he wags his tail: stealing passwords through content queries (conquer) attacks": "android accessibility service was designed to assist individuals with disabilities in using android devices. however, it has been exploited by attackers to steal user passwords due to design shortcomings. google has implemented various countermeasures to make it difficult for these types of attacks to be successful on modern android devices. in this paper, we present a new type of side channel attack called content queries (conquer) that can bypass these defenses. we discovered that android does not prevent the content of passwords from being queried by the accessibility service, allowing malware with this service enabled to enumerate the combinations of content to brute force the password. while this attack seems simple to execute, there are several challenges that must be addressed in order to successfully launch it against real-world apps. these include the use of lazy query to differentiate targeted password strings, active query to determine the right timing for the attack, and timing- and state-based side channels to infer case-sensitive passwords. our evaluation results demonstrate that the conquer attack is effective at stealing passwords, with an average one-time success rate of 64.91%. this attack also poses a threat to all android versions from 4.1 to 12, and can be used against tens of thousands of apps. in addition, we analyzed the root cause of the conquer attack and discussed several countermeasures to mitigate the potential security risks it poses.", "doitrust: dissecting on-chain compromised internet domains via graph learning": "traditional block/allow lists remain a significant defense against malicious websites, by limiting end-users' access to domain names. however, such lists are often incomplete and reactive in nature. in this work, we first introduce an expansion graph which creates organically grown internet domain allow-lists based on trust transitivity by crawling hyperlinks. then, we highlight the gap of monitoring nodes with such an expansion graph, where malicious nodes are buried deep along the paths from the compromised websites, termed as \"on-chain compromise\". the stealthiness (evasion of detection) and large-scale issues impede the application of existing web malicious analysis methods for identifying on-chain compromises within the sparsely labeled graph. to address the unique challenges of revealing the on-chain compromises, we propose a two-step integrated scheme, doitrust, leveraging both individual node features and topology analysis: (i) we develop a semi-supervised suspicion prediction scheme to predict the probability of a node being relevant to targets of compromise (i.e., the denied nodes), including a novel node ranking approach as an efficient global propagation scheme to incorporate the topology information, and a scalable graph learning scheme to separate the global propagation from the training of the local prediction model, and (ii) based on the suspicion prediction results, efficient pruning strategies are proposed to further remove highly suspicious nodes from the crawled graph and analyze the underlying indicator of compromise. experimental results show that doitrust achieves 90% accuracy using less than 1% labeled nodes for the suspicion prediction, and its learning capability outperforms existing node-based and structure-based approaches. we also demonstrate that doitrust is portable and practical. we manually review the detected compromised nodes, finding that at least 94.55% of them have suspicious content, and investigate the underlying indicator of on-chain compromise.", "double and nothing: understanding and detecting cryptocurrency giveaway scams": "as cryptocurrencies increase in popularity and users obtain and manage their own assets, attackers are pivoting from just abusing cryptocurrencies as a payment mechanism, to stealing crypto assets from end users. in this paper, we report on the first large-scale analysis of cryptocurrency giveaway scams. giveaway scams are deceptively simple scams where attackers set up webpages advertising fake events and promising users to double or triple the funds that they send to a specific wallet address. to understand the population of these scams in the wild we design and implement cryptoscamtracker, a tool that uses certificate transparency logs to identify likely giveaway scams. through a 6-month-long experiment, cryptoscamtracker identified a total of 10,079 giveaway scam websites targeting users of all popular cryptocurrencies. next to analyzing the hosting and domain preferences of giveaway scammers, we perform the first quantitative analysis of stolen funds using the public blockchains of the abused cryptocurrencies, extracting the transactions corresponding to 2,266 wallets belonging to scammers. we find that just for the scams discovered in our reporting period, attackers have stolen the equivalent of tens of millions of dollars, organizing large-scale campaigns across different cryptocurrencies. lastly, we find evidence that attackers try to re-victimize users by offering fund-recovery services and that some victims send funds multiple times to the same scammers.", "drone security and the mysterious case of dji's droneid": "consumer drones enable high-class aerial video photography, promise to reform the logistics industry, and are already used for humanitarian rescue operations and during armed conflicts.\ncontrasting their widespread adoption and high popularity, the low entry barrier for air mobility---a traditionally heavily regulated sector---poses many risks to safety, security, and privacy. malicious parties could, for example, (mis-)use drones for surveillance, transportation of illegal goods, or cause economic damage by intruding the closed airspace above airports.\nto prevent harm, drone manufacturers employ several countermeasures to enforce safe and secure use of drones, e.g., they impose software limits regarding speed and altitude, or use geofencing to implement no-fly zones around airports or prisons.\ncomplementing traditional countermeasures, drones from the market leader dji implement a tracking protocol called droneid, which is designed to transmit the position of both the drone and its operator to authorized entities such as law enforcement or operators of critical infrastructures.\nin this paper, we analyze security and privacy claims for drones, focusing on the leading manufacturer dji with a market share of 94%. we first systemize the drone attack surface and investigate an attacker capable of eavesdropping on the drone's over-the-air data traffic. based on reverse engineering of dji firmware, we design and implement a decoder for dji's proprietary tracking protocol droneid, using only cheap cots hardware. we show that the transmitted data is not encrypted, but accessible to anyone, compromising the drone operator's privacy. second, we conduct a comprehensive analysis of drone security: using a combination of reverse engineering, a novel fuzzing approach tailored to dji's communication protocol, and hardware analysis, we uncover several critical flaws in drone firmware that allow attackers to gain elevated privileges on two different dji drones and their remote control. such root access paves the way to disable or bypass countermeasures and abuse drones. in total, we found 16 vulnerabilities, ranging from denial of service to arbitrary code execution. 14 of these bugs can be triggered remotely via the operator's smartphone, allowing us to crash the drone mid-flight.", "edgetdc: on the security of time difference of arrival measurements in can bus systems": "a controller area network (can bus) is a message-based protocol for intra-vehicle communication designed mainly with robustness and safety in mind. in real-world deployments, can bus does not offer common security features such as message authentication. due to the fact that automotive suppliers need to guarantee interoperability, most manufacturers rely on a decade-old standard (iso 11898) and changing the format by introducing macs is impractical. research has therefore suggested to address this lack of authentication with can bus intrusion detection systems (idss) that augment the bus with separate modules. idss attribute messages to the respective sender by measuring physical-layer features of the transmitted frame. those features are based on timings, voltage levels, transients\u2014and, as of recently, time difference of arrival (tdoa) measurements. in this work, we show that tdoa-based approaches presented in prior art are vulnerable to novel spoofing and poisoning attacks. we describe how those proposals can be fixed and present our own method called edgetdc. unlike existing methods, edgetdc does not rely on analog-to-digital converters (adcs) with high sampling rate and high dynamic range to capture the signals at sample level granularity. our method uses time-to-digital converters (tdcs) to detect the edges and measure their timings. despite being inexpensive to implement, tdcs offer low latency, high location precision and the ability to measure every single edge (rising and falling) in a frame. measuring each edge makes analog sampling redundant and allows the calculation of statistics that can even detect tampering with parts of a message. through extensive experimentation, we show that edgetdc can successfully thwart masquerading attacks in the can system of modern vehicles.", "fine-grained trackability in protocol executions": "we introduce a new framework, trackdev, for encoding and analysing the tracing or \"tracking\" of an entity (e.g., a device) via its executions of a protocol or its usages of a system. trackdev considers multiple dimensions combined: whether the attacker is active or passive, whether an entity is trackable in its every single appearances or just in a compound set thereof, and whether the entity can be explicitly or implicitly identified.\ntrackdev can be applied to most identification-based systems. trackdev is to be applied in practice, over actual executions of systems; to this end, we test trackdev on real-life traffic for two well-known protocols, the lorawan join and the 5g handovers, showing new trackability attacks therein and proposing countermeasures.\nwe study the strength of trackdev's various trackability properties and show that many of our notions are incomparable amongst each other, thus justifying the fine-grained nature of trackdev. \nfinally, we detail how the main thrust of trackdev can be mechanised in formal-verification tools, without any loss; we exemplify this fully on the lorawan join, in  the tamarin prover.\nin this process, we also uncover and discuss within two important aspects: (a) trackdev\u2019s separation between \"explicit\" and \"implicit\" trackability offers new formal-verification insights; (b) our analyses of the lorawan join protocol in tamarin against trackdev as well as against existing approximations of unlinkability by baelde et al. concretely show that the latter approximations can be coarser than our notions.", "focusing on pinocchio's nose: a gradients scrutinizer to thwart split-learning hijacking attacks using intrinsic attributes": "split learning is privacy-preserving distributed learning that has gained momentum recently. it also faces new security challenges. fsha is a serious threat to split learning. in fsha, a malicious server hijacks training to trick clients to train the encoder of an autoencoder instead of a classification model. intermediate results sent to the server by a client are actually latent codes of private training samples, which can be reconstructed with high fidelity from the received codes with the decoder of the autoencoder. splitguard is the only existing effective defense against hijacking attacks. it is an active method that injects falsely labeled data to incur abnormal behaviors to detect hijacking attacks. such injection also incurs an adverse impact on honest training of intended models.\nin this paper, we first show that splitguard is vulnerable to an adaptive hijacking attack named splitspy. splitspy exploits the same property that splitguard exploits to detect hijacking attacks. in splitspy, a malicious server maintains a shadow model that performs the intended task to detect falsely labeled data and evade splitguard. our experimental evaluation indicates that splitspy can effectively evade splitguard. then we propose a novel passive detection method, named gradients scrutinizer, which relies on intrinsic differences between gradients from an intended model and those from a malicious model: the expected similarity among gradients of same-label samples differs from the expected similarity among gradients of different-label samples for an intended model, while they are the same for a malicious model. this intrinsic distinguishability enables gradients scrutinizer to effectively detect split-learning hijacking attacks without tampering with honest training of intended models. our extensive evaluation indicates that gradients scrutinizer can effectively thwart both known split-learning hijacking attacks and adaptive counterattacks against it.", "folk models of misinformation on social media": "in this paper we investigate what textit{folk models of misinformation} exist on social media with a sample of 235 social media users. work on social media misinformation does not investigate how ordinary users deal with it; rather, the focus is mostly on the anxiety, tensions, or divisions misinformation creates. studying only the structural aspects also overlooks how misinformation is internalized by users on social media and thus is quick to prescribe \"inoculation\" strategies for the presumed lack of immunity to misinformation. how users grapple with social media content to develop \"natural immunity\" as a precursor to misinformation resilience, however, remains an open question. we have identified at least five textit{folk models} that conceptualize misinformation as either: textit{political (counter)argumentation}, textit{out-of-context narratives}, textit{inherently fallacious information}, textit{external propaganda}, or simply textit{entertainment}. we use the rich conceptualizations embodied in these folk models to uncover how social media users minimize adverse reactions to misinformation encounters in their everyday lives.", "fuzzilli: fuzzing for javascript jit compiler vulnerabilities": "javascript has become an essential part of the internet infrastructure, and today's interactive web applications would be inconceivable without this programming language. on the downside, this interactivity implies that web applications rely on an ever-increasing amount of computationally intensive javascript code, which burdens the javascript engine responsible for efficiently executing the code. to meet these rising performance demands, modern javascript engines ship with sophisticated just-in-time (jit) compilers. however, jit compilers are a complex technology and, consequently, provide a broad attack surface for potential faults that might even be security-critical.\nprevious work on discovering software faults in javascript engines found many vulnerabilities, often using fuzz testing. unfortunately, these fuzzing approaches are not designed to generate source code that actually triggers jit semantics. consequently, jit vulnerabilities are unlikely to be discovered by existing methods.\nin this paper, we close this gap and present the first fuzzer that focuses on jit vulnerabilities.\nmore specifically, we present the design and implementation of an intermediate representation (ir) that focuses on discovering jit compiler vulnerabilities. we implemented a complete prototype of the proposed approach and evaluated our fuzzer over a period of six months. in total, we discovered 17 confirmed security vulnerabilities. our results show that targeted jit fuzzing is possible and a dangerously neglected gap in fuzzing coverage for javascript engines.", "he-htlc: revisiting incentives in htlc": "hashed time-locked contracts (htlcs) are a widely used primitive in blockchain systems such as payment channels, atomic swaps, etc. unfortunately, htlc is incentive-incompatible and is vulnerable to bribery attacks.\nthe state-of-the-art solution is mad-htlc (oakland'21), which proposes an elegant idea that leverages miners' profit-driven nature to defeat bribery attacks.\nin this paper, we show that mad-htlc is still vulnerable as it only considers a somewhat narrow set of textit{passive} strategies by miners. through a family of novel textit{reverse-bribery} attacks, we show concrete textit{active} strategies that miners can take to break mad-htlc and profit at the loss of mad-htlc users. for these attacks, we present their implementation and game-theoretical profitability analysis.\nbased on the learnings from our attacks, we propose a new htlc realization, he-htlc (our specification is lightweight and inert to incentive manipulation attacks. hence, we call it he-htlc where he stands for helium.) that is provably secure against all possible strategic manipulation (passive and active). in addition to being secure in a stronger adversary model, he-htlc achieves other desirable features such as low and user-adjustable collateral, making it more practical to implement and use the proposed schemes. we implemented he-htlc on bitcoin and the transaction cost of he-htlc is comparative to average bitcoin transaction fees.", "heteroscore: evaluating and mitigating cloud security threats brought by heterogeneity": "cloud computing has emerged as a critical part of commercial computing infrastructure due to its computing power, data storage capabilities, scalability, software/api integration, and convenient billing features. at the early stage of cloud computing, the majority of clouds are homogeneous, i.e., most machines are identical. it has been proven that heterogeneity in the cloud, where a variety of machine configurations exist, provides higher performance and power efficiency for applications. this is because heterogeneity enables applications to run in more suitable hardware/software environments. in recent years, the adoption of heterogeneous cloud has increased with the integration of a variety of hardware into cloud systems to serve the requirements of increasingly diversified user applications.\nat the same time, the emergence of security threats, such as micro-architectural attacks, is becoming a more critical problem for cloud users and providers. it has been demonstrated (e.g., repttack and cloak & co-locate) that the prerequisite of micro-architectural attacks, the co-location of attack and victim instances, is easier to achieve in the heterogeneous cloud. this also means that the ease of attack is not just related to the heterogeneity of the cloud but increases with the degree of heterogeneity. however, there is a lack of numerical metrics to define, quantify or compare the heterogeneity of one cloud environment with another. in this paper, we propose a novel metric called heterogeneity score (heteroscore), which quantitatively evaluates the heterogeneity of a cluster. we demonstrate that heteroscore is closely connected to security against co-location attacks. furthermore, we propose mitigation techniques to trade-off heterogeneity offered with security. we believe this is the first quantitative study that evaluates cloud heterogeneity and links heterogeneity to infrastructure security.", "him of many faces: characterizing billion-scale adversarial and benign browser fingerprints on commercial websites": "browser fingerprints, while traditionally being used for web tracking, have recently been adopted more and more often for defense or detection of various attacks targeting real-world websites. faced with these situations, adversaries also upgrade their weapons to generate their own fingerprints---defined as adversarial fingerprints---to bypass existing defense or detection. naturally, such adversarial fingerprints are different from benign ones from user browsers because they are generated intentionally for defense bypass. however, no prior works have studied such differences in the wild by comparing adversarial with benign fingerprints let alone how adversarial fingerprints are generated.\nin this paper, we present the first billion-scale measurement study of browser fingerprints collected from 14 major commercial websites (all ranked among alexa/tranco top 10,000). we further classify these fingerprints into either adversarial or benign using a learning-based, feedback-driven fraud and bot detection system from a major security company, and then study their differences. our results draw three major observations: (i) adversarial fingerprints are significantly different from benign ones in many metrics, e.g., entropy, unique rate, and evolution speed, (ii) adversaries are adopting various tools and strategies to generate adversarial fingerprints, and (iii) adversarial fingerprints vary across different attack types, e.g., from content scraping to fraud transactions.", "how to count bots in longitudinal datasets of ip addresses": "estimating the size of a botnet is one of the most basic and important queries one can make when trying to understand the impact of a botnet. surprisingly and unfortunately, this seemingly simple task has confounded many measurement efforts. while it may seem tempting to simply count the number of ip addresses observed to be infected, it is well-known that doing so can lead to drastic overestimates, as isps commonly assign new ip addresses to hosts. as a result, estimating the number of infected hosts given longitudinal datasets of ip addresses has remained an open problem.\nin this paper, we present a new data analysis technique, cardcount, that provides more accurate size estimations by accounting for ip address reassignments. cardcount can be applied on longer windows of observations than prior approaches (weeks compared to hours), and is the first technique of its kind to provide confidence intervals for its size estimations. we evaluate cardcount on three real world datasets and show that it performs equally well to existing solutions on synthetic ideal situations, but drastically outperforms all previous work in realistic botnet situations. for the hajime and mirai botnets, we estimate that cardcount, is 51.6% and 69.1% more accurate than the state of the art techniques when estimating the botnet size over a 28-day window.", "i still know what you watched last sunday: privacy of the hbbtv protocol in the european smart tv landscape": "the ever-increasing popularity of smart tvs and support for the hybrid broadcast broadband tv (hbbtv) standard allow broadcasters to enrich content offered to users via the standard broadcast signal with internet-delivered apps, e.g., ranging from quizzes during a tv show to targeted advertisement. hbbtv works using standard web technologies as transparent overlays over a tv channel. despite the number of hbbtv-enabled devices rapidly growing, studies on the protocol's security and privacy aspects are scarce, and no standard protective measure is in place. \nwe fill this gap by investigating the current state of hbbtv in the european landscape and assessing its implications for users' privacy. we shift the focus from the smart tv's firmware and app security, already studied in-depth in related work, to the content transmission protocol itself. contrary to traditional ``linear tv'' signals, hbbtv allows for bi-directional communication: in addition to receiving tv content, it also allows for transmitting data back to the broadcaster. we describe techniques broadcasters use to measure users' (viewing) preferences and show how the protocol's implementation can cause severe privacy risks by studying its deployment by 36 tv channels in five european countries (italy, germany, france, austria, and finland). we also survey users' awareness of smart tv and hbbtv-related risks. our results show little understanding of the possible threats users are exposed to. finally, we present a denylist-based mechanism to ensure a safe experience for users when watching tv and to reduce the privacy issues that hbbtv may pose.", "infomasker: preventing eavesdropping using phoneme-based noise": "with the wide deployment of microphone-equipped smart devices, more and more users have concerns that their voices would be secretly recorded. recent studies show that microphones have nonlinearity and can be jammed by inaudible ultrasound, which leads to the emergence of ultrasonic-based anti-eavesdropping research. however, existing solutions are implemented through energetic masking and require high energy to disturb human voice. since ultrasonic noise can only remain inaudible at limited energy, such noise can merely cover a short distance and can be easily removed by adversaries, which makes these solutions impractical. in this paper, we explore the idea of informational masking, study the transmission and coverage constraints of ultrasonic jamming, and implement a highly effective anti-eavesdropping system, named infomasker. specifically, we design a phoneme-based noise that is robust against denoising methods and can effectively prevent both humans and machines from understanding the jammed signals. we optimize the ultrasonic transmission method to achieve higher transmission energy and lower signal distortion, then implement a prototype of our system. experimental results show that infomasker can effectively reduce the accuracy of all tested speech recognition systems to below 50% even at low energies (snr=0), which is much better than existing noise designs.", "loki: state-aware fuzzing framework for the implementation of blockchain consensus protocols": "blockchain consensus protocols are responsible for coordinating the nodes to make agreements on the transaction results.\ntheir implementation bugs, including\nmemory-related and consensus logic vulnerabilities, may pose serious threats.\nfuzzing is a promising technique for protocol vulnerability detection.\nhowever, existing fuzzers cannot deal with complex consensus states of distributed nodes, thus generating a large number of useless packets, inhibiting their effectiveness in reaching the deep logic of consensus protocols.\nin this work, we propose loki, a blockchain consensus protocol fuzzing framework that detects the consensus memory-related and logic bugs. loki senses consensus states in real-time by masquerading as a node. first, loki dynamically builds a state model that records the state transition of each node. after that, loki adaptively generates the input targets, types, and contents according to the state model. with a bug analyzer, loki detects the consensus protocol implementation bugs with well-defined oracles.\nwe implemented and evaluated loki on four widely used commercial blockchain systems, including go-ethereum, facebook diem, ibm fabric, and webank fisco-bcos.\nloki has detected 20 serious previously unknown vulnerabilities with 9 cves assigned. 14 of them are memory-related bugs, and 6 are consensus logic bugs.\ncompared with state-of-the-art tools such as peach, fluffy, and twins, loki improves the branch coverage by an average of 43.21%, 182.05%, and 291.58%.", "metawave: attacking mmwave sensing with meta-material-enhanced tags": "millimeter-wave (mmwave) sensing has been applied in many critical applications, serving millions of thousands of people around the world. however, it is vulnerable to attacks in the real world. these attacks are based on expensive and professional radio frequency (rf) modulator-based instruments and can be prevented by conventional practice (e.g., rf fingerprint). in this paper, we propose and design a novel passive mmwave attack, called metawave, with low-cost and easily obtainable meta-material tags for both vanish and ghost attack types. these meta-material tags are made of commercial off-the-shelf (cots) materials with customized tag designs to attack various goals, which considerably low the attack bar on mmwave sensing. specifically, we demonstrate that tags made of ordinal material (e.g., c-ram lf) can be leveraged to precisely tamper the mmwave echo signal and spoof the range, angle, and speed sensing measurements. besides, to optimize the attack, a general simulator-based metawave attack framework is proposed and designed to simulate the tag modulation effects on the mmwave signal with advanced tag and scene parameters. we evaluate, metawave, the meta-material tag attack in both simulation and real-world experiments (i.e., 20 different environments) with various attack settings. experimental results demonstrate that metawave can achieve up to 97% top-1 attack accuracy on range estimation, 96% on angle estimation, and 91% on speed estimation in actual practice, 10-100x cheaper than existing mmwave attack methods. we also evaluate the usability and robustness of metawave under different real-world scenarios. moreover, we conduct in-depth analysis and discussion on countermeasures for metawave mmwave attacks to improve wireless sensing and cyber-infrastructure security.", "navigating murky waters: automated browser feature testing for uncovering tracking vectors": "modern web browsers constitute complex application platforms with a wide range of apis and features. critically, this includes a multitude of heterogeneous mechanisms that allow sites to store information that explicitly or implicitly alters client-side state or functionality. this behavior implicates any browser storage, cache, access control, and policy mechanism as a potential tracking vector. as demonstrated by prior work, tracking vectors can manifest through elaborate behaviors and exhibit varying characteristics that differ vastly across different browsing\ncontexts. in this paper we develop canitrack, an automated, mechanism-agnostic framework for testing browser features and uncovering novel tracking vectors. our system is designed for facilitating browser vendors and researchers by streamlining the systematic testing of browser mechanisms. it accepts methods to read and write entries for a mechanism and calls these methods across different browsing contexts to determine any potential tracking vulnerabilities that the mechanism may expose. to demonstrate our system\u2019s capabilities we test 21 browser mechanisms and uncover a slew of tracking vectors, including 13 that enable third-party tracking and two that bypass the isolation offered by private browsing modes. importantly, we show how two separate mechanisms from google\u2019s highly-publicized and widely-discussed privacy sandbox initiative can be leveraged for tracking. our experimental findings have resulted in 20 disclosure reports across seven major browsers, which have set remediation efforts in motion. overall, our study highlights the complex and formidable challenge that browsers currently face when trying to balance the adoption of new features and protecting the privacy of their users, as well as the potential benefit of incorporating canitrack into their internal testing pipeline.", "no grammar, no problem: towards fuzzing the linux kernel without system-call descriptions": "the integrity of the entire computing ecosystem depends on the security of our operating systems (oses). unfortunately, due to the scale and complexity of os code, hundreds of security issues are found in oses, every year. as such, operating systems have constantly been prime use-cases for applying security-analysis tools. in recent years, fuzz-testing has appeared as the dominant technique for automatically finding security issues in software. as such, fuzzing has been adapted to find thousands of bugs in kernels. however, modern os fuzzers, such as syzkaller, rely on precise, extensive, manually created harnesses and grammars for each interface fuzzed within the kernel. due to this reliance on grammars, current os fuzzers are faced with scaling-issues. \nin this paper, we present fuzzng, our generic approach to fuzzing system-calls on oses. unlike syzkaller, fuzzng does not require intricate descriptions of system-call interfaces in order to function. instead fuzzng leverages fundamental kernel design features in order to reshape and simplify the fuzzer\u2019s input-space. as such fuzzng only requires a small config, for each new target: essentially a list of files and system-call numbers the fuzzer should explore. \nwe implemented fuzzng for the linux kernel. testing fuzzng over 10 linux components with extensive descrip tions in syzkaller showed that, on average, fuzzng achieves 102.5% of syzkaller\u2019s coverage. fuzzng found 9 new bugs (5 in components that syzkaller had already fuzzed extensively, for years). additionally, fuzzng\u2019s lightweight configs are less than 1.7% the size of syzkaller\u2019s manually-written grammars. crucially, fuzzng achieves this without initial seed-inputs, or expert guidance.", "obi: a multi-path oblivious ram for forward-and-backward-secure searchable encryption": "dynamic searchable encryption (dse) is a user-cloud protocol for searching over outsourced encrypted data. many current dse schemes resort to oblivious rams (oram) to achieve forward privacy and backward privacy, which is a concept to describe security levels of the protocol. we show that, however, most prior oram-based dse suffers from a new problem: it is inefficient to fetch/insert a large set of data blocks. we call this the large-stash eviction problem. to address the problem, we present obi, a multi-path oblivious ram, which accesses multiple tree paths per query for handling a large set of data blocks. we classify traditional tree-based orams as single-path orams if they access a single path per query. obi has two new high-throughtput multi-path eviction algorithms that are several orders of magnitude more efficient than the well-known path-oram eviction algorithm when the stash is large. we prove that the proposed multi-path oram outperforms the traditional single-path oram in terms of local stash size and insertion efficiency. security analysis shows that obi is secure under the strong forward and backward security model. obi can protect the well-known dse leakage, such as the search pattern and the size pattern. we also show that obi can be applied to oblivious file systems and oblivious conjunctive-query dse schemes. we conduct experiments on the enron dataset. the experimental results demonstrate that obi is far more efficient than the state-of-the-art oram-based dse schemes.", "obsan: an out-of-bound sanitizer to harden dnn executables": "the rapid adoption of deep neural network (dnn) models on a variety of hardware platforms has boosted the development of deep learning (dl) compilers. dl compilers take as input the high-level dnn model specifications and generate optimized dnn executables for diverse hardware architectures like cpus and gpus. despite the emerging adoption of dl compilers in real-world scenarios, no solutions exist to protect dnn executables. to fill this critical gap, this paper introduces obsan, a fast sanitizer designed to check out-of-bound (oob) behavior of dnn executables. from a holistic view, dnn incorporates bidirectional computation: forward propagation that predicts an output based on an input, and backward propagation that characterizes how the forward prediction is made. both neuron activations in forward propagation and the gradients in backward propagation should fall within valid ranges, and deviations from the valid ranges would be considered as oob.\noob is primarily related to unsafe behavior of dnns, which root from anomalous inputs and may cause mispredictions or even exploitation via adversarial examples (aes). we thus design obsan, which includes two variants, fobsan and bobsan, that can detect oob in the forward and backward propagations, respectively. each obsan is designed as extra passes of dl compilers to integrate with large-scale dnn models, and we design various optimization schemes to reduce the overhead of obsan. evaluations over various anomalous inputs show that obsan manifests promising oob detectability with low overhead. we further present two downstream applications to show how obsan prevents online ae generation and facilitates feedback-driven fuzz testing toward dnn executables.", "optrand: optimistically responsive reconfigurable distributed randomness": "public random beacons publish random numbers at regular intervals, which anyone can obtain and verify. the design of public distributed random beacons has been an exciting research direction with significant implications for blockchains, voting, and beyond. distributed random beacons, in addition to being bias-resistant and unpredictable, also need to have low communication overhead and latency, high resilience to faults, and ease of reconfigurability. existing synchronous random beacon protocols sacrifice one or more of these properties. \nin this work, we design an efficient unpredictable synchronous random beacon protocol, optrand, with quadratic (in the number $n$ of system nodes) communication complexity per beacon output. first, we innovate by employing a novel combination of bilinear pairing based publicly verifiable secret-sharing and non-interactive zero-knowledge proofs to build a linear (in $n$) sized publicly verifiable random sharing. second, we develop a state machine replication protocol with linear-sized inputs that is also optimistically responsive, i.e., it can progress responsively at actual network speed during optimistic conditions, despite the synchrony assumption, and thus incur low latency. in addition, we present an efficient reconfiguration mechanism for optrand that allows nodes to leave and join the system. our experiments show our protocols perform significantly better compared to state-of-the-art protocols under optimistic conditions and on par with state-of-the-art protocols in the normal case. we are also the first to implement a reconfiguration mechanism for distributed beacons and demonstrate that our protocol continues to be live during reconfigurations.", "parakeet: practical key transparency for end-to-end encrypted messaging": "encryption alone is not enough for secure end-to-end encrypted messaging: a server must also honestly serve public keys to users. key transparency has been presented as an efficient solution for detecting (and hence deterring) a server that attempts to dishonestly serve keys. key transparency involves two major components: (1) a username to public key mapping, stored and cryptographically committed to by the server, and, (2) an out-of-band consistency protocol for serving short commitments to users. in the setting of real-world deployments and supporting production scale, new challenges must be considered for both of these components. we enumerate these challenges and provide solutions to address them. in particular, we design and implement a memory-optimized and privacy-preserving verifiable data structure for committing to the username to public key store.\nto make this implementation viable for production, we also integrate support for persistent and distributed storage. we also propose a future-facing solution, termed \"compaction'', as a mechanism for mitigating practical issues that arise from dealing with infinitely growing server data structures. finally, we implement a consensusless solution that achieves the minimum requirements for a service that consistently distributes commitments for a transparency application, providing a much more efficient protocol for distributing small and consistent commitments to users. this culminates in our production-grade implementation of a key transparency system (parakeet) which we have open-sourced, along with a demonstration of feasibility through our benchmarks.", "paralyzing drones via emi signal injection on sensory communication channels": "an inertial measurement unit (imu) takes the key responsibility for the attitude control of drones. it is comprised of various sensors and transfers sensor data to the drones\u2019 control unit. if it reports incorrect data, the drones cannot maintain their attitude and will crash down to the ground. thus, several anti-drone studies have focused on causing significant fluctuations in the imu sensor data by resonating the mechanical structure of the internal sensors using a crafted acoustic wave. however, this approach is limited in terms of efficacy for several reasons. as the structural details of each sensor in an imu significantly differ by type, model, and manufacturer, the attack needs to be conducted independently for each sensor. furthermore, it can be easily mitigated by using other supplementary sensors that are not corrupted by the attack or even with cheap plastic shielding.\nin this paper, we propose a novel anti-drone technique that effectively corrupts any imu sensor data regardless of the sensor\u2019s type, model, and manufacturer. our key idea is to distort the communication channel between the imu and control unit of a drone by using an electromagnetic interference (emi) signal injection. experimentally, for a given control unit board, regardless\nof the sensor used, we discovered a distinct susceptible frequency at which an emi signal can greatly distort the sensor data. compared to a general em pulse (emp) attack, it requires much less power as it targets the specific susceptible frequency. it can also avoid collateral damage from the emp attack. for practical evaluation, we demonstrate the feasibility of the attack using real drones; the attack instantly paralyzed the drones. lastly, we conclude by presenting practical challenges for its mitigation.", "partitioning ethereum without eclipsing it": "we present a practical partitioning attack, which we call gethlighting, that isolates an ethereum full node from the rest of the network for hours without having to occupy (or eclipse) all of the target\u2019s peer connections. in gethlighting, an adversary controls only about a half (e.g., 25 out of total 50) of all peer connections of a target node, achieving powerful partitioning with a small attack budget of operating several inexpensive virtual machines. at the core of gethlighting, its low-rate denial-of-service (dos) strategy effectively stops the growth of local blockchain for hours while leaving other ethereum node operations undisturbed. we analyze how subtle and insignificant delays incurred by a low-rate dos can lead to a powerful blockchain partitioning attack. the practical impact of gethlighting is discussed \u2014 i.e., the attack is scalable and low-cost (only about $5,714 for targeting all ethereum full nodes concurrently for 24 hours), and extremely simple to launch. we demonstrate the feasibility of gethlighting with full nodes in the ethereum mainnet and testnet in both controlled and real-world experiments. we identify a number of fundamental system characteristics in ethereum that enable gethlighting attacks and propose countermeasures that require some protocol and client implementation enhancements. ethereum foundation has acknowledged this vulnerability in september 2022 and one of our countermeasures has been accepted as a hotfix for geth 1.11.0.", "preventing sim box fraud using device model fingerprinting": "sim boxes have been playing a critical role in the underground ecosystem of international-scale frauds that steal billions of dollars from individual victims and mobile network operators across the globe.\nmany mitigation schemes have been proposed for these frauds, mainly aiming to detect fraud call sessions; however, one direct approach to this problem---the prevention of the sim box devices from network use---has not drawn much attention despite its highly anticipated benefit.\nthis is exactly what we aim to achieve in this paper.\nwe propose a simple access control logic that detects when unauthorized sim boxes use cellular networks for communication.\nat the heart of our defense proposal is the precise fingerprinting of device models (eg, distinguishing an iphone 13 from any other smartphone models on the market) and device types (ie, smartphones and iot devices) without relying on international mobile equipment identity, which can be spoofed easily.\nwe empirically show that fingerprints, which were constructed from network-layer auxiliary information with more than 31k features, are mostly distinct among 85 smartphones and thus can be used to prevent the vast majority of illegal sim boxes from making unauthorized voice calls.\nour proposal, as the very first practical, reliable unauthorized cellular device model detection scheme, greatly simplifies the mitigation against sim box frauds.", "privacy-preserving database fingerprinting": "when sharing   relational databases with other parties, in addition to providing high quality (utility) database to the recipients, a database owner also aims to have (i) privacy guarantees for the data entries and (ii)  liability guarantees (via fingerprinting) in case of unauthorized redistribution. however, (i) and (ii) are orthogonal objectives, because when sharing a database with multiple recipients, privacy via data sanitization requires adding noise once (and sharing the same noisy version with all recipients), whereas liability via unique fingerprint insertion requires adding different noises to each shared copy to distinguish all recipients. although achieving (i) and (ii) together is possible in a na\"ive way (e.g., either differentially-private  database perturbation or synthesis followed by fingerprinting), this approach  results in   significant degradation in the utility of   shared databases. in this paper, we achieve privacy and liability guarantees  simultaneously by proposing a novel entry-level differentially-private  (dp) fingerprinting mechanism for relational databases without causing large utility  degradation. \nthe proposed mechanism fulfills the privacy and liability requirements by leveraging the randomization nature of  fingerprinting and transforming it into provable privacy guarantees.\nspecifically, we devise a bit-level random response scheme to achieve differential privacy guarantee for arbitrary data entries when sharing the entire database, and then, based on this, we develop an $epsilon$-entry-level dp fingerprinting mechanism. we theoretically analyze the connections between privacy, fingerprint robustness, and database utility by  deriving closed form expressions. we also propose a sparse vector technique-based solution to control the cumulative privacy loss when fingerprinted copies of a database are shared with multiple recipients. \nwe experimentally show that our mechanism achieves strong  fingerprint robustness (e.g., the fingerprint cannot be compromised even if the malicious database recipient  modifies/distorts more than half of the entries in  its received   fingerprinted copy), and higher database utility compared to various baseline methods (e.g., application-dependent database utility of the shared database achieved  by the proposed mechanism is     higher than that of the considered baselines).", "probflow : using probabilistic programming in anonymous communication networks": "we present probflow, a probabilistic programming approach for estimating relay capacities in the tor network. we refine previously derived probabilistic model of the network to take into account more of the complexity of the real-world tor network. we use this model to perform inference in a probabilistic programming language called numpyro which allows us to overcome the analytical barrier present in purely analytical approach. we integrate the implementation of probflow to the\ncurrent implementation of capacity estimation algorithms in the tor network. we demonstrate the practical benefits of probflow by simulating it in flow-based python simulator and packet-based shadow simulations, the highest fidelity simulator available for the tor network. in both simulators, probflow provides significantly more accurate estimates that results in improved user performance, with average download speeds increasing by 25% in the shadow simulations.", "rai2: responsible identity audit governing the artificial intelligence": "identity plays an important role in responsible artificial intelligence (ai): it acts as a unique marker for deep learning (dl) models and can be used to trace those accountable for irresponsible use of models. consequently, effective dl identity audit is fundamental for building responsible ai. besides models, training datasets determine what features a model can learn, and thus should be paid equal attention in identity audit. in this work, we propose the first practical scheme, named rai2, for responsible identity audit for both datasets and models. we develop our dataset and model similarity estimation methods that can work with black-box access to suspect models. the proposed methods can quantitatively determine the identity of datasets and models by estimating the similarity between the owner's and suspect's. finally, we realize our responsible audit scheme based on the commitment scheme, enabling the owner to register datasets and models to a trusted third party (ttp) which is in charge of dataset and model regulation and forensics of copyright infringement. extensive evaluation on 14 model architectures and 6 visual and textual datasets shows that our scheme can accurately identify the dataset and model with the proposed similarity estimation methods. we hope that our audit methodology will not only fill the gap in achieving identity arbitration but also ride on the wave of ai governance in this chaotic world.", "reaas: enabling adversarially robust downstream classifiers via robust encoder as a service": "encoder as a service is an emerging cloud service. specifically, a service provider first pre-trains an encoder (i.e., a general-purpose feature extractor) via either supervised learning or self-supervised learning and then deploys it as a cloud service api. a client queries the cloud service api to obtain feature vectors for its training/testing inputs when training/testing its classifier (called downstream classifier). a downstream classifier is vulnerable to adversarial examples, which are testing inputs with carefully crafted perturbation that the downstream classifier misclassifies. therefore, in safety and security critical applications, a client aims to build a robust downstream classifier and certify its robustness guarantees against adversarial examples. \n what apis should the cloud service provide, such that a client can use any certification method to certify the  robustness of its downstream classifier against adversarial examples while minimizing the number of queries to the apis? how can a service provider pre-train an encoder such that clients can build more certifiably robust downstream classifiers? we aim to answer the two questions in this work. for the first question, we show that the cloud service only needs to provide two apis, which we carefully design, to enable a client to certify the robustness of its downstream classifier with a minimal number of queries to the apis. for the second question, we show that an encoder pre-trained using a spectral-norm regularization term enables clients to build more robust downstream classifiers.", "real threshold ecdsa": "threshold ecdsa recently regained popularity due to decentralized applications such as dnssec and cryptocurrency asset custody. latest (communication-optimizing) schemes often assume all n or at least n' >= t participating users remain honest throughout the pre-signing phase, essentially degenerating to n'-out-of-n' multiparty signing instead of t-out-of-n threshold signing. when anyone misbehaves, all signers must restart from scratch, rendering prior computation and communication in vain. this hampers the adoption of threshold ecdsa in time-critical situations and confines its use to a small signing committee.\nto mitigate such denial-of-service vulnerabilities prevalent in state-of-the-art, we propose a robust threshold ecdsa scheme that achieves the t-out-of-n threshold flexibility \"for real\" throughout the whole pre-signing and signing phases without assuming an honest majority. our scheme is desirable when computational resources are scarce and in a decentralized setting where faults are easier to be induced. our design features 4-round pre-signing, o(n) cheating identification, and self-healing machinery over distributive shares. prior arts mandate abort after an o(n^2)-cost identification, albeit with 3-round pre-signing (canetti et al., ccs '20), or o(n) using 6 rounds (castagnos et al., tcs '23). empirically, our scheme saves up to ~30% of the communication cost, depending on at which stage the fault occurred.", "redsec: running encrypted discretized neural networks in seconds": "machine learning as a service (mlaas) has risen to become a prominent technology due to the large development time, amount of data, hardware costs, and level of expertise required to develop a machine learning model. however, privacy concerns prevent the adoption of mlaas for applications with sensitive data. a promising privacy preserving solution is to use fully homomorphic encryption (fhe) to perform the ml computations. recent advancements have lowered computational costs by several orders of magnitude, opening doors for secure practical applications to be developed. in this work, we introduce the redsec framework that optimizes fhe-based private machine learning inference by leveraging ternary neural networks. such neural networks, whose weights are constrained to {-1,0,1}, have special properties that we exploit to operate efficiently in the homomorphic domain. redsec introduces novel features, including a new data re-use scheme that enables bidirectional bridging between the integer and binary domains for the first time in fhe. this enables us to implement very efficient binary operations for multiplication and activations, as well as efficient integer domain additions. our approach is complemented by a new gpu acceleration library, dubbed (red)cufhe, which supports both binary and integer operations on multiple gpus. redsec brings unique benefits by supporting user-defined models as input (bring-your-own-network), automation of plaintext training, and efficient evaluation of private inference leveraging tfhe. in our analysis, we perform inference experiments with the mnist, cifar-10, and imagenet datasets and report performance improvements compared to related works.", "rescan: a middleware framework for realistic and robust black-box web application scanning": "black-box web vulnerability scanners are invaluable for security researchers and practitioners. despite recent approaches tackling emph{some} of the inherent limitations of scanners, many have not sufficiently evolved alongside web browsers and applications, and often lack the capabilities for handling the inherent challenges of navigating and interacting with modern web applications. instead of building an alternative scanner that could naturally only incorporate a limited set of the wide range of vulnerability-finding capabilities offered by the multitude of existing scanners, in this paper we propose an entirely different strategy. we present rescan, a emph{scanner-agnostic} middleware framework that emph{transparently} enhances scanners' capabilities by mediating their interaction with web applications in a realistic and robust manner, using an orchestrated, fully-fledged modern browser. in essence, our framework can be used in conjunction with emph{any} vulnerability scanner, thus allowing users to benefit from the capabilities of existing and future scanners.  our extensible and modular framework includes a collection of enhancement techniques that address limitations and obstacles commonly faced by state-of-the-art scanners. our experimental evaluation demonstrates that despite the considerable (and expected) overhead introduced by a fully-fledged browser, our framework significantly improves the code coverage achieved by popular scanners (168% on average), resulting in a 66% and 161% increase in the number of reflected and stored xss vulnerabilities detected, respectively.", "rr: a fault model for efficient tee replication": "trusted execution environments (tees) ensure the confidentiality and integrity of computations in hardware. subject to the tee's threat model, the hardware shields a computation from most externally induced fault behavior except crashes. as a result, a crash-fault tolerant (cft) replication protocol should be sufficient when replicating trusted code inside tees.  however, tees do not provide efficient and general means of ensuring the freshness of external, persistent state. therefore, cft replication is insufficient for tee computations with external state, as this state could be rolled back to an earlier version when a tee restarts.  furthermore, using bft protocols in this setting is too conservative, because these protocols are designed to tolerate arbitrary behavior, not just rollback during a restart.\nin this paper, we propose the restart-rollback (rr) fault model for replicating tees, which precisely captures the possible fault behaviors of tees with external state. then, we show that existing replication protocols can be easily adapted to this fault model with few changes, while retaining their original performance. we adapted two widely used crash fault tolerant protocols - the abd read/write register protocol and the paxos consensus protocol - to the rr model.  furthermore, we leverage these protocols to build a replicated metadata service called emph{teems}, and then show that it can be used to add tee-grade confidentiality, integrity, and freshness to untrusted cloud storage services.  our evaluation shows that our protocols perform significantly better than their bft counterparts (between $1.25$ and $55times$ better throughput), while performing identically to the cft versions, which do not protect against rollback attacks.", "sometimes, you aren\u2019t what you do: mimicry attacks against provenance graph host intrusion detection systems": "reliable methods for host-layer intrusion detection remained an open problem within computer security. recent research has recast intrusion detection as a provenance graph anomaly detection problem thanks to concurrent advancements in machine learning and causal graph auditing. while these approaches show promise, their robustness against an adaptive adversary has yet to be proven. in particular, it is unclear if mimicry attacks, which plagued past approaches to host intrusion detection, have a similar effect on modern graph-based methods.\nin this work, we reveal that systematic design choices have allowed mimicry attacks to continue to abound in provenance graph host intrusion detection systems (prov-hids). against a corpus of exemplar prov-hids, we develop evasion tactics that allow attackers to hide within benign process behaviors. evaluating against public datasets, we demonstrate that an attacker can consistently evade detection (100% success rate) without modifying the underlying attack behaviors. we go on to show that our approach is feasible in live attack scenarios and outperforms domain-general adversarial sample techniques. through open sourcing our code and datasets, this work will serve as a benchmark for the evaluation of future prov-hids.", "soundlock: a novel user authentication scheme for vr devices using auditory-pupillary response": "virtual reality (vr) has shown promising potential in many applications, such as e-business, healthcare, and social networking. rich information regarding users' activities and online accounts is stored in vr devices. if {they are} carelessly unattended, adversarial access will cause data breaches and other critical consequences. practical user authentication schemes for vr devices are in dire need. current solutions, including passwords, digital pins, and pattern locks, mostly follow conventional approaches for general personal devices. they have been criticized for deficits in both security and usability. in this work, we propose soundlock, a novel user authentication scheme for vr devices using auditory-pupillary response as biometrics. during authentication, auditory stimuli are presented to the user via the vr headset. the corresponding pupillary response is captured by the integrated eye tracker. user's legitimacy is then determined by comparing the response with the template generated during the enrollment stage. to strike a balance between security and usability in the scheme design, an optimization problem is formulated. due to its non-linearity, a two-stage heuristic algorithm is proposed to solve it efficiently. the solution provides necessary guidance for selecting effective auditory stimuli and determining their corresponding lengths. we demonstrate through extensive in-field experiments that soundlock outperforms state-of-the-art biometric solutions with far (frr) as low as 0.76% (0.91%) and is well received among participants in the user study.", "stealthyimu: stealing permission-protected private information from smartphone voice assistant using zero-permission sensors": "voice user interfaces (vuis) are becoming an indispensable module that enables hands-free interaction between human users and smartphones. unfortunately, recent research revealed a side channel that allows zero-permission motion sensors to eavesdrop on the vui voices from the co-located smartphone loudspeaker. nonetheless, these threats are limited to leaking a small set of digits and hot words. in this paper, we propose stealthyimu, a new threat that uses motion sensors to steal permission-protected private information from the vuis. we develop a set of efficient models to detect and extract private information, taking advantage of the deterministic structures in the vui responses. our experiments show that stealthyimu can steal private information from 23 types of frequently-used voice commands to acquire contacts, search history, calendar, home address, and even gps trace with high accuracy. we further propose effective mechanisms to defend against stealthyimu without noticeably impacting the user experience.", "synthdb: synthesizing database via program analysis for security testing of web applications": "testing database-backed web applications is challenging because their behaviors (e.g., control flow) are highly dependent on data returned from sql queries. without a database containing sufficient and realistic data, it is challenging to reach potentially vulnerable code snippets, limiting various existing dynamic-based security testing approaches. however, obtaining such a database for testing is difficult in practice as it often contains sensitive information. sharing it can lead to data leaks and privacy issues.\nin this paper, we present synthdb, a program analysis-based database generation technique for database-backed php applications. synthdb leverages a concolic execution engine to identify interactions between php codebase and the sql queries. it then collects and solves various constraints to reconstruct a database that can enable exploring uncovered program paths without violating database integrity. our evaluation results show that the database generated by synthdb outperforms state-of-the-arts database generation techniques  in terms of code and query coverage in 17 real-world php applications. specifically, synthdb generated databases achieve 62.9% code and 77.1% query coverages, which are 14.0% and 24.2% more in code and query coverages than the state-of-the-art techniques. furthermore, our security analysis results show that synthdb effectively aids existing security testing tools: burp suite, wfuzz, and webfuzz. burp suite aided by synthdb detects 76.8% of vulnerabilities while other existing techniques cover 55.7% or fewer. impressively, with synthdb, burp suite discovers 33 previously unknown vulnerabilities from 5 real-world applications.", "the power of bamboo: on the post-compromise security for searchable symmetric encryption": "dynamic searchable symmetric encryption (dsse) enables users to delegate the keyword search over dynamically updated encrypted databases to an honest-but-curious server without losing keyword privacy. this paper studies a new and practical security risk to dsse, namely, secret key compromise (e.g., a user's secret key is leaked or stolen), which threatens all the security guarantees offered by existing dsse schemes. to address this open problem, we introduce the notion of searchable encryption with key-update (seku) that provides users with the option of non-interactive key updates. we further define the notion of post-compromise secure with respect to leakage functions to study whether dsse schemes can still provide data security after the client's secret key is compromised. we demonstrate that post-compromise security is achievable with a proposed protocol called ``bamboo\". interestingly, the leakage functions of bamboo satisfy the requirements for both forward and backward security. we conduct a performance evaluation of bamboo using a real-world dataset and compare its runtime efficiency with the existing forward-and-backward secure dsse schemes. the result shows that bamboo provides strong security with better or comparable performance.", "thwarting smartphone sms attacks at the radio interface layer": "the short message service (sms) is a cornerstone of modern smartphone communication that enables inter-personal text messaging and other sms-based services (e.g., two-factor authentication). however, it can also be readily exploited to compromise unsuspecting remote victims. for instance, novel exploits such as simjacker and wibattack enable transmission of binary sms messages that could surreptitiously execute dangerous commands on a victim device. the sms channel may also be subverted to drive other nefarious activities (e.g., spamming, dos, and tracking), thereby undermining end-user security and privacy. unfortunately, neither contemporary smartphone operating systems nor existing defense techniques provide a comprehensive bulwark against the spectrum of evolving sms-driven threats. to address this limitation, we develop a novel defense framework called rildefender, which to the best of our knowledge is the first inline prevention system integrated into the radio interface layer (ril) of android smartphones. we describe an implementation of rildefender on three smartphone models with five android versions of the android open source project (aosp), and show that it is able to protect users from six types of sms attacks spanning four adversary models. we evaluate rildefender against 19 reproduced sms attacks and 11 contemporary sms malware samples and find that rildefender detects all and automatically prevents all but one of these threats without affecting normal cellular operations.", "un-rocking drones: foundations of acoustic injection attacks and recovery thereof": "drones equipped with microelectromechanical system (mems) inertial measurement unit (imu) sensors are exposed to acoustic injection attacks. these attacks resonate sensors, compromising their output and causing drones to crash. several mitigation strategies have been proposed; however, they are limited in terms of practicality as they cannot make the drone fly to its planned destination in the event of an attack. \nto remedy this, we aim at recovering the compromised sensor values for the practical mitigation of acoustic injection attacks. to achieve this, we first constructed a realistic testbed and delved into the implications of resonant mems sensors on drones. we discovered that sampling jitter, which refers to the inconsistent timing delay in retrieving sensor values, has a significant impact on drone crashes during the attack. note that while any real-time system needs to satisfy its real-time requirements, it does have sampling jitter owing to manufacturing errors or scheduling/operational overheads. the sampling jitter is negligible in terms of real-time requirements; however, we found that it became critical for drones being attacked. this is because the sampling jitter spreads the resonant sensor signals into the in-band range of the drones\u2019 control logic, thereby neutralizing the drones\u2019 safety mechanisms, such as a low-pass filter.\nconsidering the resonant signals affected by sampling jitter as noise, we developed a novel mitigation strategy that leverages a noise reduction technique, namely a denoising autoencoder. this approach recovers benign sensor signals from compromised ones for the resonant mems imu sensors, without requiring other supplementary sensors. we implemented this prototype, termed unrocker , and demonstrated its capability through a series of experiments reflecting real-world scenarios. to facilitate future research, we released our source code and experimental data.", "vulhawk: cross-architecture vulnerability detection with entropy-based binary code search": "code reuse is widespread in software development. it brings a heavy spread of vulnerabilities, threatening software security. unfortunately, with the development and deployment of the internet of things (iot), the harms of code reuse are magnified. binary code search is a viable way to find these hidden vulnerabilities. facing iot firmware images compiled by different compilers with different optimization levels from different architectures, the existing methods are hard to fit these complex scenarios. in this paper, we propose a novel intermediate representation function model, which is an architecture-agnostic model for cross-architecture binary code search. it lifts binary code into microcode and preserves the main semantics of binary functions via complementing implicit operands and pruning redundant instructions. then, we use natural language processing techniques and graph convolutional networks to generate function embeddings. we call the combination of a compiler, architecture, and optimization level as a file environment, and take a divide-and-conquer strategy to divide a similarity calculation problem of $c_n^2$ cross-file-environment scenarios into n-1 embedding transferring sub-problems. we propose an entropy-based adapter to transfer function embeddings from different file environments into the same file environment to alleviate the differences caused by various file environments. to precisely identify vulnerable functions, we propose a progressive search strategy to supplement function embeddings with fine-grained features to reduce false positives caused by patched functions. we implement a prototype named vulhawk and conduct experiments under seven different tasks to evaluate its performance and robustness. the experiments show vulhawk outperforms asm2vec, asteria, bindiff, gmn, palmtree, safe, and trex."}